{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ajay200.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "435d36aa-a3ee-4d0e-a925-7e2e87153aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=2edb15ae83a0ad3e2a4213c6b9e06c44b3d1c9358d977c2d22c1edda673d2497\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "ffedee54-35ad-421a-c9dc-b752c4ca49a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "ec8447a7-a3b2-4efb-b101-e8c3efeafe6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "940365a4-c574-4339-c1e2-d53933010980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "36a4c59d-4beb-4c22-bd3c-6408617ee634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "63b7c062-838a-4ef0-8df5-08785960149e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019801\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -20.009603\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.019507\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.019312\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.019119\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.018928\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.028738\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.028451\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.028166\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.037885\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.047506\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.057031\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.066461\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.075796\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.085038\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.094188\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.083246\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.072413\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.081689\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.090872\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.099964\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.098964\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.107974\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.116894\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.125726\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.124468\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.123224\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.131991\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.140671\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.139265\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.147872\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.156393\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.154829\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.163281\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.171648\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.179932\n",
            "resetting env. episode 40.000000, reward total was -19.000000. running mean: -20.168133\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.166451\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -20.154787\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.153239\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.151706\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.160189\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.168587\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.156902\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.165333\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.173679\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.181942\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.190123\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -20.178222\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.186440\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.194575\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.202629\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.200603\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.198597\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.206611\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.204545\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.212500\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.200375\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.198371\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.206387\n",
            "resetting env. episode 64.000000, reward total was -18.000000. running mean: -20.184323\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.192480\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.200555\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.198550\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.206564\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.214499\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.202354\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.200330\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -20.188327\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.186443\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.194579\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.202633\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.200607\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.198601\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.206615\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.214549\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.212403\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.210279\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.218176\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.205995\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.193935\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.181995\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.190175\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.188274\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.196391\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.204427\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.202383\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.210359\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.218255\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.226073\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -20.213812\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.221674\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.219457\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.227263\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.234990\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.222640\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.220414\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.228210\n",
            "resetting env. episode 102.000000, reward total was -18.000000. running mean: -20.205927\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.213868\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.221729\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.229512\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.227217\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.234945\n",
            "resetting env. episode 108.000000, reward total was -17.000000. running mean: -20.202595\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.210569\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.218464\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.226279\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.234016\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.241676\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.249259\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.256767\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.264199\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.271557\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.278842\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.286053\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.283193\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.290361\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.297457\n",
            "resetting env. episode 123.000000, reward total was -18.000000. running mean: -20.274483\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.271738\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.279020\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.286230\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.293368\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.300434\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.307430\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.314356\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.321212\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.308000\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.304920\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.311871\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.298752\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.305764\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.312707\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.319580\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.326384\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.333120\n",
            "resetting env. episode 141.000000, reward total was -19.000000. running mean: -20.319789\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.326591\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.323325\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.330092\n",
            "resetting env. episode 145.000000, reward total was -18.000000. running mean: -20.306791\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.313723\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.320586\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.327380\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.324106\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.330865\n",
            "resetting env. episode 151.000000, reward total was -18.000000. running mean: -20.307556\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.314481\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.321336\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.328123\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.324841\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.331593\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.328277\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.334994\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.341644\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.328228\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.334946\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.321596\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.328380\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.335096\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.331745\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.338428\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.345044\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.341593\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.348177\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.354696\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.361149\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.367537\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.353862\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.360323\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.366720\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.373053\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.359322\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.355729\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.362172\n",
            "resetting env. episode 180.000000, reward total was -18.000000. running mean: -20.338550\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.345164\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.341713\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.348296\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.354813\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.361265\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.357652\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.364075\n",
            "resetting env. episode 188.000000, reward total was -19.000000. running mean: -20.350435\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.356930\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.363361\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.369727\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.366030\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.372370\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.378646\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.374860\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.371111\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.367400\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.373726\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.379989\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.376189\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.372427\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.378703\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.374916\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.361166\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.367555\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.373879\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.380140\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.386339\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.382476\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.378651\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.384864\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.371016\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.347306\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.353833\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.350294\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.356791\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.363223\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.369591\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.375895\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.372136\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.378415\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.384631\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.390784\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.386877\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.393008\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.399078\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.405087\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.411036\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.406926\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.402857\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.398828\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.404840\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.400791\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.386783\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.392916\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.398986\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.394997\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.401047\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.407036\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.412966\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.408836\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.414748\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.410600\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.416494\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.422329\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.428106\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.433825\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.439487\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.425092\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.430841\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.426532\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.422267\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.408044\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.413964\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.399824\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.405826\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.411768\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.417650\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.423474\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.419239\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.415047\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.420896\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.416687\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.422520\n",
            "resetting env. episode 265.000000, reward total was -18.000000. running mean: -20.398295\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.404312\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.410269\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.406166\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.412105\n",
            "resetting env. episode 270.000000, reward total was -18.000000. running mean: -20.387984\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.394104\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.390163\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.386261\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.372399\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.378675\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.384888\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.391039\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.397129\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.393157\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.379226\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.385433\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.391579\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.397663\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.393687\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.399750\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.405752\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.411695\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.407578\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.393502\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.399567\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.405571\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.391516\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.397600\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.403624\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.409588\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.405492\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.411437\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.407323\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.393250\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.399317\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.405324\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.411271\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.417158\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.422987\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.428757\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.434469\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.430124\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.435823\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.431465\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.437150\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.432779\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.428451\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.434167\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.419825\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.415627\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.421470\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.427256\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.432983\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.438653\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.444267\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.449824\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.455326\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.460773\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.456165\n",
            "resetting env. episode 325.000000, reward total was -17.000000. running mean: -20.421603\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.427387\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.433113\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.438782\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.444394\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.449950\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.455451\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.460896\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.456287\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.461725\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.457107\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.462536\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.467911\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.473232\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.468499\n",
            "resetting env. episode 340.000000, reward total was -19.000000. running mean: -20.453814\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.459276\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.454684\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.460137\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.465535\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.460880\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.466271\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.461608\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.456992\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.462422\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.467798\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.463120\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.458489\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.453904\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.439365\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.444971\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.450522\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.436017\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.441656\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.447240\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.452767\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.458240\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.453657\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.459121\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.464530\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.469884\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.465185\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.470534\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.475828\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.481070\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.476259\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.481497\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.486682\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.491815\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.486897\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.482028\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.487207\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.492335\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.497412\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.502438\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.507414\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.512339\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.517216\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.522044\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.526823\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.511555\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.516440\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.501275\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.506262\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.501200\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.496188\n",
            "resetting env. episode 391.000000, reward total was -18.000000. running mean: -20.471226\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.456514\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.461949\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.467329\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.472656\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.477929\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.463150\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.468518\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.473833\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.469095\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.454404\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.449860\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.455361\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.450808\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.446300\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.451837\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.447318\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.452845\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.458317\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.453733\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.459196\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.464604\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.469958\n",
            "resetting env. episode 414.000000, reward total was -17.000000. running mean: -20.435259\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.430906\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.436597\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.432231\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.437909\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.433530\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.439194\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.444802\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.450354\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.445851\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.451392\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.436878\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.442510\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.448084\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.453604\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.459068\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.444477\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.440032\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.445632\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.441175\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.446764\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.422296\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.428073\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.423792\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.409554\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.405459\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.391404\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.387490\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.393615\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.399679\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.385682\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.391826\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.397907\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.403928\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.409889\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.405790\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.411732\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.417615\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.423439\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.419204\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.405012\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.400962\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.396953\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.402983\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.408953\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.404864\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.410815\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.416707\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.422540\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.428314\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.414031\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.419891\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.425692\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.421435\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.417221\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.423049\n",
            "resetting env. episode 470.000000, reward total was -19.000000. running mean: -20.408818\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.404730\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.410683\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.416576\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.412410\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.418286\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.424103\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.419862\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.415663\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.421507\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.427292\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.433019\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.428689\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.434402\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.430058\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.425757\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.421500\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.427285\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.413012\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.408882\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.404793\n",
            "resetting env. episode 491.000000, reward total was -18.000000. running mean: -20.380745\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.376937\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.383168\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.369336\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.355643\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.362087\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.358466\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.354881\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.351332\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.347819\n",
            "CPU times: user 24min 46s, sys: 11min 40s, total: 36min 27s\n",
            "Wall time: 18min 56s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "404a774c-db38-4088-d3b6-06af2dba341c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990394\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.990490\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.990585\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.980679\n",
            "resetting env. episode 10.000000, reward total was -18.000000. running mean: -20.950873\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.951364\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.941850\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.922432\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.913207\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.904075\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.905035\n",
            "resetting env. episode 17.000000, reward total was -18.000000. running mean: -20.875984\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.867224\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.858552\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.849967\n",
            "resetting env. episode 21.000000, reward total was -19.000000. running mean: -20.831467\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.833152\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.834821\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.836473\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.818108\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.819927\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.821727\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.823510\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.825275\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.807022\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.808952\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.810863\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.812754\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.814626\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.816480\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.798315\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.800332\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.802329\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.804306\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.806263\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.808200\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -20.790118\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.792217\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.794295\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.786352\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.768488\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.770803\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.753095\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.745564\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.738109\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.740728\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.743320\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.745887\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.748428\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.730944\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.733634\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.726298\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.729035\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.711745\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.714627\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.717481\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.710306\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.703203\n",
            "resetting env. episode 64.000000, reward total was -18.000000. running mean: -20.676171\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.659409\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.652815\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.656287\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.659724\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.663127\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.656496\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.649931\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.653432\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.656897\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.650328\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.653825\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.657287\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.660714\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.664107\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.657466\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.660891\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.664282\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.667639\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.670963\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.674253\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.667511\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.670836\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.664127\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.667486\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.670811\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.674103\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.677362\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.680588\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.673782\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -20.657045\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.650474\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.653969\n",
            "resetting env. episode 97.000000, reward total was -17.000000. running mean: -20.617430\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.621255\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.625043\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.628792\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.622505\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.626279\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.620017\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.623817\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -20.607578\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.611503\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.615388\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.609234\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.603141\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.607110\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.611039\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.614928\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.618779\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.612591\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.606465\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.600401\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.604397\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.608353\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.612269\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.616147\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.619985\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.623785\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.627547\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.611272\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.605159\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.609108\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.603017\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.606986\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.610917\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.614807\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.618659\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.612473\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.616348\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.620185\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.623983\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.627743\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.621465\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -20.605251\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.589198\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.593306\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.587373\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.591499\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.595584\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.589629\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.593732\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.587795\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.591917\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.585998\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.590138\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.594237\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.598294\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.602311\n",
            "resetting env. episode 153.000000, reward total was -17.000000. running mean: -20.566288\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.560625\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -20.545019\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.549569\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.554073\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.558532\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.562947\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.567318\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.571644\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.555928\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.550369\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.544865\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.549416\n",
            "resetting env. episode 166.000000, reward total was -18.000000. running mean: -20.523922\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.528683\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.533396\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.538062\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.542682\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.547255\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.551782\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.556264\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.560702\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.565095\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.559444\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.563849\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.558211\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.542629\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.537202\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.541830\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.536412\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.541048\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.535637\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.530281\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.514978\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.499829\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.494830\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.499882\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.484883\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.480034\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.475234\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.460482\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.445877\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.441418\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.437004\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.442634\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.438207\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.443825\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.449387\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.434893\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.430544\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.436239\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.441877\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.447458\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.452983\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.448453\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.453969\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.459429\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.464835\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.450186\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.455685\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.451128\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.436616\n",
            "resetting env. episode 215.000000, reward total was -18.000000. running mean: -20.412250\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.418128\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.423947\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.429707\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.435410\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.431056\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.436745\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.442378\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.447954\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.453475\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.458940\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.464350\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.469707\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.455010\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.450460\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.455955\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.451396\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.456882\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.442313\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.447890\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.433411\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.439077\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.444686\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.450239\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.445737\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.451279\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.456767\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.462199\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.467577\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.472901\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.478172\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.483390\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -20.458556\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.463971\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.469331\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.454638\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.460091\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.455491\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.460936\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.466326\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.451663\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.457146\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.462575\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.467949\n",
            "resetting env. episode 259.000000, reward total was -18.000000. running mean: -20.443270\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.428837\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.434549\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.440203\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.445801\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.451343\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.456830\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.462261\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.457639\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.453062\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.458532\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.453946\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.439407\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.445013\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.440563\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.446157\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.451696\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.457179\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.462607\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.467981\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.473301\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.468568\n",
            "resetting env. episode 281.000000, reward total was -18.000000. running mean: -20.443882\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.429443\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.435149\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.410798\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.416690\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.422523\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.428297\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.424014\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.429774\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.435477\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.441122\n",
            "resetting env. episode 292.000000, reward total was -19.000000. running mean: -20.426711\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.432443\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.438119\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.443738\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.429300\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.425007\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.420757\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.426550\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.422284\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.428061\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.423781\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.429543\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.425248\n",
            "resetting env. episode 305.000000, reward total was -18.000000. running mean: -20.400995\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.396985\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.403015\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.408985\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.414895\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.410746\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.416639\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.402473\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.398448\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.394463\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.380519\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.386714\n",
            "resetting env. episode 317.000000, reward total was -18.000000. running mean: -20.362846\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.359218\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.355626\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.352069\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.338549\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.335163\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.341812\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.348394\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.354910\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.361361\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.367747\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.374069\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.380329\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.386525\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.392660\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.378734\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.374946\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.381197\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.367385\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.373711\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.379974\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.386174\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.382312\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.378489\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.374704\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.380957\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.377148\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.383376\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.379543\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.375747\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.381990\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.388170\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.394288\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.400345\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.406342\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.412278\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.408156\n",
            "resetting env. episode 354.000000, reward total was -19.000000. running mean: -20.394074\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.380133\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.376332\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.372569\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.368843\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.375154\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.371403\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.377689\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.383912\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.380073\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.356272\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.352709\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.339182\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.345790\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.342333\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.348909\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.345420\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.341966\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.328546\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.335261\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.341908\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.348489\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.345004\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.351554\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.348039\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.354558\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.351013\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.357503\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.353928\n",
            "resetting env. episode 383.000000, reward total was -18.000000. running mean: -20.330388\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.337084\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.343714\n",
            "resetting env. episode 386.000000, reward total was -17.000000. running mean: -20.310276\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.297174\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.304202\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.311160\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.308048\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.304968\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.301918\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.288899\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.286010\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.283150\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.290318\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.297415\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.294441\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.291497\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.298582\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.305596\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.312540\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.319414\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.326220\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.322958\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.329729\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.326431\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.333167\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.339835\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.346437\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.342973\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.339543\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.336147\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.342786\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.349358\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.345864\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.352406\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.358882\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.355293\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.361740\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.348123\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.344641\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.331195\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.337883\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.344504\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.341059\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.347649\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.354172\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.360630\n",
            "resetting env. episode 430.000000, reward total was -18.000000. running mean: -20.337024\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.333654\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.340317\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.346914\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.353445\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.359911\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.356311\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.352748\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.359221\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.365629\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.361972\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.368353\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.374669\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.370922\n",
            "resetting env. episode 444.000000, reward total was -17.000000. running mean: -20.337213\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.333841\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.340503\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.337098\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.333727\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.340389\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.346985\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.333516\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.340180\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.346779\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.353311\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.359778\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.366180\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.362518\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.368893\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.375204\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.381452\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.377638\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.373861\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.370123\n",
            "resetting env. episode 464.000000, reward total was -17.000000. running mean: -20.336421\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.343057\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.349627\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.356130\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.352569\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.349043\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.355553\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.361997\n",
            "resetting env. episode 472.000000, reward total was -18.000000. running mean: -20.338377\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.334994\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.341644\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.348227\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.354745\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.361197\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.357585\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.354010\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.350470\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.346965\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.343495\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.350060\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.346560\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.343094\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.349663\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.346166\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.352705\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.339178\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.335786\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.332428\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.329104\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.325813\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -20.302555\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.299529\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.306534\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.293468\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.280534\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.277728\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.284951\n",
            "CPU times: user 25min 35s, sys: 11min 58s, total: 37min 34s\n",
            "Wall time: 19min 26s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "c366329d-0f5a-4ac1-c0c2-ca33dbc2f19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "e690c427-5b5b-4d75-8ab6-14213965c3fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.980199\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980397\n",
            "resetting env. episode 6.000000, reward total was -18.000000. running mean: -20.950593\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.951087\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951576\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.952060\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.952540\n",
            "resetting env. episode 11.000000, reward total was -18.000000. running mean: -20.923014\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.913784\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.894646\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.875700\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.876943\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.878174\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.869392\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.870698\n",
            "resetting env. episode 19.000000, reward total was -18.000000. running mean: -20.841991\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.833571\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.825235\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.816983\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.818813\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.820625\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.812419\n",
            "resetting env. episode 26.000000, reward total was -19.000000. running mean: -20.794295\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.786352\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.788488\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.780603\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.782797\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.774969\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.767220\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.759547\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.761952\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.764332\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.756689\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.759122\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.761531\n",
            "resetting env. episode 39.000000, reward total was -18.000000. running mean: -20.733916\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.726576\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.719311\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.722118\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.724896\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.727647\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.730371\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.713067\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.715937\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.698777\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.691789\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.694872\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.697923\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.690944\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.674034\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.667294\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.670621\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.673915\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.677176\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.680404\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.673600\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.666864\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.670195\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.673493\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.676758\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.679991\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.673191\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.676459\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.679694\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.682897\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -20.666068\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.669408\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.672714\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.675986\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.659227\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.652634\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.656108\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.659547\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.652951\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.656422\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.659858\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.653259\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.646727\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.650259\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.653757\n",
            "resetting env. episode 84.000000, reward total was -19.000000. running mean: -20.637219\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.640847\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.644438\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.627994\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.611714\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.615597\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.619441\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.613247\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.607114\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.611043\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.604933\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.608883\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.612794\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.606666\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.610600\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.614494\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.618349\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.622165\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.605944\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.609884\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.613785\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.607648\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.611571\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.615455\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.609301\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.613208\n",
            "resetting env. episode 110.000000, reward total was -18.000000. running mean: -20.587076\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.591205\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.585293\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.589440\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.583546\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.587710\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.581833\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.586015\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.590155\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.594253\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.598310\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.592327\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.596404\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.590440\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.594536\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.598590\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.602604\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.586578\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.590713\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.584805\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.568957\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.563268\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.567635\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.571959\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.566239\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.570577\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.574871\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.579122\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.583331\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.577498\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.581723\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.575906\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.580147\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.584345\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.588502\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.592617\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.596690\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.600724\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.604716\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.598669\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.602682\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.596656\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.580689\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.584882\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.569033\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.573343\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.577610\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.581833\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.586015\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.580155\n",
            "resetting env. episode 160.000000, reward total was -18.000000. running mean: -20.554353\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.538810\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.543422\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.547988\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.552508\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -20.526983\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.521713\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.516496\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.521331\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.516117\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.520956\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.515747\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.510589\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.505483\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.510429\n",
            "resetting env. episode 175.000000, reward total was -19.000000. running mean: -20.495324\n",
            "resetting env. episode 176.000000, reward total was -18.000000. running mean: -20.470371\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.455667\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.461111\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.466499\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.471834\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.477116\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.482345\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.467522\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.472846\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.478118\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.473337\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.468603\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.463917\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.469278\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.474585\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.469839\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.475141\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.480390\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.485586\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.490730\n",
            "resetting env. episode 196.000000, reward total was -18.000000. running mean: -20.465823\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.471164\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.466453\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.461788\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.467170\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.462499\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.467874\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.463195\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.468563\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.463877\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.459239\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.464646\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.450000\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.445500\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.441045\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.436634\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.432268\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.427945\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.413666\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.419529\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.425334\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.431081\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.436770\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.442402\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.447978\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.443498\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.449063\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.444573\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.440127\n",
            "resetting env. episode 225.000000, reward total was -18.000000. running mean: -20.415726\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.411568\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.407453\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.413378\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.409244\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.405152\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.411100\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.416989\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.422819\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.418591\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.414405\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.420261\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.416059\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.411898\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.407779\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.413701\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.409564\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.415469\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.421314\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.407101\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.413030\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.398900\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.394911\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.400961\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.396952\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.402982\n",
            "resetting env. episode 251.000000, reward total was -19.000000. running mean: -20.388953\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.395063\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.401112\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.397101\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.403130\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.409099\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.405008\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.410958\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.416848\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.412680\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.408553\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.414467\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.410323\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.396220\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.402257\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.408235\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.414152\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.410011\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.405911\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.411852\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.417733\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.403556\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.409520\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.405425\n",
            "resetting env. episode 275.000000, reward total was -19.000000. running mean: -20.391371\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.397457\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.393483\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.399548\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.395552\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.381597\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.387781\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.373903\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.380164\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.386362\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.392499\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.398574\n",
            "resetting env. episode 287.000000, reward total was -18.000000. running mean: -20.374588\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.370842\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.377134\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.383362\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.389529\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.395633\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.401677\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.407660\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.413584\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.409448\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.405353\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.401300\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.407287\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.413214\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.419082\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.404891\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.410842\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.406734\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.412666\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.418540\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.424354\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.420111\n",
            "resetting env. episode 309.000000, reward total was -18.000000. running mean: -20.395910\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.401951\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.397931\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.393952\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.400012\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.386012\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.382152\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.388330\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.394447\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.400503\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.406498\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.402433\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.388408\n",
            "resetting env. episode 322.000000, reward total was -18.000000. running mean: -20.364524\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.360879\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.367270\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.373598\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.359862\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.366263\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.352600\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.349074\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.355584\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.352028\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.338507\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.325122\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.331871\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.328552\n",
            "resetting env. episode 336.000000, reward total was -18.000000. running mean: -20.305267\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.312214\n",
            "resetting env. episode 338.000000, reward total was -18.000000. running mean: -20.289092\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.276201\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.273439\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.270705\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.267998\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.265318\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.272665\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.279938\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.287139\n",
            "resetting env. episode 347.000000, reward total was -18.000000. running mean: -20.264267\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.271624\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.258908\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.256319\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.263756\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.271118\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.268407\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.265723\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.273066\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.280335\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.277532\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.264757\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.262109\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.269488\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.276793\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.274025\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.261285\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.258672\n",
            "resetting env. episode 365.000000, reward total was -19.000000. running mean: -20.246085\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.253624\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.251088\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.258577\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.255992\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.263432\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.260797\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.268189\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.265507\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.262852\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.250224\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.257722\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.255144\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.262593\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.269967\n",
            "resetting env. episode 380.000000, reward total was -18.000000. running mean: -20.247267\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.254795\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.262247\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.269624\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.276928\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.284159\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.291317\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.298404\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.305420\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.312366\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.319242\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.326050\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.332789\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.329461\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.336167\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.342805\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.339377\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.345983\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.352523\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.338998\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.345608\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.352152\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.358631\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.355044\n",
            "resetting env. episode 404.000000, reward total was -19.000000. running mean: -20.341494\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.348079\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.344598\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.351152\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.357641\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.344064\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.340624\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.347217\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.353745\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.350208\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.346706\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.343239\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.339806\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.346408\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.342944\n",
            "resetting env. episode 419.000000, reward total was -17.000000. running mean: -20.309515\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.296419\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.303455\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.310421\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.307316\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.314243\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.321101\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.317890\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.314711\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.321564\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.328348\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.335065\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.341714\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.338297\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.344914\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.331465\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.338150\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.324769\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.331521\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.328206\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.324924\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.321674\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.328458\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.335173\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.321821\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.318603\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.325417\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.332163\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.338841\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.335453\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.322098\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.318877\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.305689\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.312632\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.309505\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.306410\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.313346\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.310213\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.317111\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.323940\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.320700\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.317493\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.324318\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.331075\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.337764\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.344387\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.350943\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.357433\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.363859\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.370220\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.376518\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.382753\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.388926\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.385036\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.381186\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.387374\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.393500\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.399565\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.405570\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.411514\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.407399\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.403325\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.409292\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.405199\n",
            "resetting env. episode 483.000000, reward total was -17.000000. running mean: -20.371147\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -20.347435\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.333961\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.330621\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.337315\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.333942\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.330603\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.337296\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.333924\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.330584\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.327278\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.324006\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.320766\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.327558\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.324282\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.331040\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.317729\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.324552\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.321306\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.328093\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.334812\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.341464\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.338050\n",
            "resetting env. episode 506.000000, reward total was -18.000000. running mean: -20.314669\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.311522\n",
            "resetting env. episode 508.000000, reward total was -19.000000. running mean: -20.298407\n",
            "resetting env. episode 509.000000, reward total was -20.000000. running mean: -20.295423\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.292469\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.299544\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.306549\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.313483\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.320348\n",
            "resetting env. episode 515.000000, reward total was -18.000000. running mean: -20.297145\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.294173\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.301232\n",
            "resetting env. episode 518.000000, reward total was -21.000000. running mean: -20.308219\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.315137\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.321986\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.328766\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.335478\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.322124\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -20.308902\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.315813\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.322655\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.329429\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.326134\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.332873\n",
            "resetting env. episode 530.000000, reward total was -17.000000. running mean: -20.299544\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.306549\n",
            "resetting env. episode 532.000000, reward total was -19.000000. running mean: -20.293483\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.300548\n",
            "resetting env. episode 534.000000, reward total was -20.000000. running mean: -20.297543\n",
            "resetting env. episode 535.000000, reward total was -19.000000. running mean: -20.284568\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.291722\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.298805\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.295817\n",
            "resetting env. episode 539.000000, reward total was -18.000000. running mean: -20.272858\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.280130\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.287329\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.294455\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.301511\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.298496\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.295511\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.292556\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.299630\n",
            "resetting env. episode 548.000000, reward total was -19.000000. running mean: -20.286634\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.293767\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.290830\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.297921\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.304942\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.311893\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.318774\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.325586\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.322330\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.319107\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.325916\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -20.322657\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.319430\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.316236\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.313073\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.319943\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.316743\n",
            "resetting env. episode 565.000000, reward total was -18.000000. running mean: -20.293576\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.300640\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.307634\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.304557\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.301512\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.308497\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.315412\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.322258\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.329035\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.325745\n",
            "resetting env. episode 575.000000, reward total was -19.000000. running mean: -20.312487\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.319362\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.316169\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.323007\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.319777\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.326579\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -20.313313\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.320180\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.316978\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.323809\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.330571\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.337265\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.343892\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.350453\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.356949\n",
            "resetting env. episode 590.000000, reward total was -20.000000. running mean: -20.353379\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.359846\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.366247\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.362585\n",
            "resetting env. episode 594.000000, reward total was -19.000000. running mean: -20.348959\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.345469\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.342014\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.348594\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.345108\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.351657\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.358141\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.354559\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.351014\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.357504\n",
            "resetting env. episode 604.000000, reward total was -18.000000. running mean: -20.333929\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.340589\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.347183\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.353712\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.350174\n",
            "resetting env. episode 609.000000, reward total was -18.000000. running mean: -20.326673\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.333406\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.340072\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.326671\n",
            "resetting env. episode 613.000000, reward total was -18.000000. running mean: -20.303404\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.300370\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.307367\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.304293\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.311250\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.318138\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.314956\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.321807\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.318589\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.315403\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.322249\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.329026\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.335736\n",
            "resetting env. episode 626.000000, reward total was -19.000000. running mean: -20.322379\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -20.309155\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.306063\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.313003\n",
            "resetting env. episode 630.000000, reward total was -20.000000. running mean: -20.309873\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.316774\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.323606\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.330370\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.327066\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.323796\n",
            "resetting env. episode 636.000000, reward total was -18.000000. running mean: -20.300558\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.297552\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.304577\n",
            "resetting env. episode 639.000000, reward total was -20.000000. running mean: -20.301531\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.298516\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.295530\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.292575\n",
            "resetting env. episode 643.000000, reward total was -18.000000. running mean: -20.269649\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.276953\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.284183\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.291342\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.278428\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.285644\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.292787\n",
            "resetting env. episode 650.000000, reward total was -19.000000. running mean: -20.279860\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.287061\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.294190\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.291248\n",
            "resetting env. episode 654.000000, reward total was -19.000000. running mean: -20.278336\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.275553\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.272797\n",
            "resetting env. episode 657.000000, reward total was -19.000000. running mean: -20.260069\n",
            "resetting env. episode 658.000000, reward total was -18.000000. running mean: -20.237468\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.235094\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.242743\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.250315\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.247812\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.255334\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.252781\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.250253\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.257750\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.245173\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.252721\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.260194\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.267592\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.274916\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.272167\n",
            "resetting env. episode 673.000000, reward total was -19.000000. running mean: -20.259445\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.256851\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.254282\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.251739\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.249222\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -20.236730\n",
            "resetting env. episode 679.000000, reward total was -19.000000. running mean: -20.224363\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.222119\n",
            "resetting env. episode 681.000000, reward total was -18.000000. running mean: -20.199898\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.207899\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.205820\n",
            "resetting env. episode 684.000000, reward total was -21.000000. running mean: -20.213762\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -20.201624\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.209608\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.207512\n",
            "resetting env. episode 688.000000, reward total was -19.000000. running mean: -20.195437\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.203482\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.211447\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.219333\n",
            "resetting env. episode 692.000000, reward total was -18.000000. running mean: -20.197140\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.205168\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.213116\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.210985\n",
            "resetting env. episode 696.000000, reward total was -18.000000. running mean: -20.188875\n",
            "resetting env. episode 697.000000, reward total was -20.000000. running mean: -20.186987\n",
            "resetting env. episode 698.000000, reward total was -19.000000. running mean: -20.175117\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.183366\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.181532\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.189717\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.187819\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.195941\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.203982\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.211942\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.219823\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.217624\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.225448\n",
            "resetting env. episode 709.000000, reward total was -19.000000. running mean: -20.213194\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.221062\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.228851\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.236563\n",
            "resetting env. episode 713.000000, reward total was -19.000000. running mean: -20.224197\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.231955\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.229635\n",
            "resetting env. episode 716.000000, reward total was -19.000000. running mean: -20.217339\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.225166\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.222914\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.230685\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -20.228378\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.236094\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.243733\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.251296\n",
            "resetting env. episode 724.000000, reward total was -19.000000. running mean: -20.238783\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.246395\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.243931\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.251492\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.248977\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.256487\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.263922\n",
            "resetting env. episode 731.000000, reward total was -19.000000. running mean: -20.251283\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.258770\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.266183\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.263521\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.260886\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.268277\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -20.265594\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.272938\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.270209\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.277507\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.274732\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.281984\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.289164\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.296273\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -20.293310\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.300377\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.297373\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.304399\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -20.291355\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.288442\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.295557\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.292602\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.299676\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.296679\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.303712\n",
            "resetting env. episode 756.000000, reward total was -19.000000. running mean: -20.290675\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.297768\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.304791\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.311743\n",
            "resetting env. episode 760.000000, reward total was -18.000000. running mean: -20.288625\n",
            "resetting env. episode 761.000000, reward total was -18.000000. running mean: -20.265739\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.263082\n",
            "resetting env. episode 763.000000, reward total was -18.000000. running mean: -20.240451\n",
            "resetting env. episode 764.000000, reward total was -18.000000. running mean: -20.218046\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.215866\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -20.213707\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -20.201570\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -20.199555\n",
            "resetting env. episode 769.000000, reward total was -20.000000. running mean: -20.197559\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.195583\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -20.183628\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.191791\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.199873\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -20.197875\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.195896\n",
            "resetting env. episode 776.000000, reward total was -21.000000. running mean: -20.203937\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.211898\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.209779\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.207681\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.205604\n",
            "resetting env. episode 781.000000, reward total was -19.000000. running mean: -20.193548\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.191612\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.199696\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.207699\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.215622\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.213466\n",
            "resetting env. episode 787.000000, reward total was -20.000000. running mean: -20.211331\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.209218\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.217126\n",
            "resetting env. episode 790.000000, reward total was -18.000000. running mean: -20.194955\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.193005\n",
            "resetting env. episode 792.000000, reward total was -19.000000. running mean: -20.181075\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.189264\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.187372\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.195498\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.193543\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.191608\n",
            "resetting env. episode 798.000000, reward total was -19.000000. running mean: -20.179692\n",
            "resetting env. episode 799.000000, reward total was -19.000000. running mean: -20.167895\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.176216\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.174454\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.182709\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.190882\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.188973\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.197083\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.205113\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.213061\n",
            "resetting env. episode 808.000000, reward total was -19.000000. running mean: -20.200931\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.198921\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.196932\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -20.184963\n",
            "resetting env. episode 812.000000, reward total was -17.000000. running mean: -20.153113\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.151582\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.160066\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.168466\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.176781\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.175013\n",
            "resetting env. episode 818.000000, reward total was -19.000000. running mean: -20.163263\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.161630\n",
            "resetting env. episode 820.000000, reward total was -19.000000. running mean: -20.150014\n",
            "resetting env. episode 821.000000, reward total was -17.000000. running mean: -20.118514\n",
            "resetting env. episode 822.000000, reward total was -19.000000. running mean: -20.107329\n",
            "resetting env. episode 823.000000, reward total was -18.000000. running mean: -20.086256\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.095393\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.094439\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.093495\n",
            "resetting env. episode 827.000000, reward total was -19.000000. running mean: -20.082560\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.091734\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.100817\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -20.089809\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -20.078911\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.088121\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.097240\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.096268\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.105305\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.114252\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.113110\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.121978\n",
            "resetting env. episode 839.000000, reward total was -19.000000. running mean: -20.110759\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.109651\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.118555\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.127369\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.136095\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.144734\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.153287\n",
            "resetting env. episode 846.000000, reward total was -18.000000. running mean: -20.131754\n",
            "resetting env. episode 847.000000, reward total was -19.000000. running mean: -20.120437\n",
            "resetting env. episode 848.000000, reward total was -19.000000. running mean: -20.109232\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.118140\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.126959\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.135689\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.144332\n",
            "resetting env. episode 853.000000, reward total was -20.000000. running mean: -20.142889\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.151460\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.159945\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.168346\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.176662\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.174896\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.173147\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.181415\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.189601\n",
            "resetting env. episode 862.000000, reward total was -17.000000. running mean: -20.157705\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.156128\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -20.154567\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.153021\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.151491\n",
            "resetting env. episode 867.000000, reward total was -18.000000. running mean: -20.129976\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.128676\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.137390\n",
            "resetting env. episode 870.000000, reward total was -19.000000. running mean: -20.126016\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.134755\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.133408\n",
            "resetting env. episode 873.000000, reward total was -19.000000. running mean: -20.122074\n",
            "resetting env. episode 874.000000, reward total was -18.000000. running mean: -20.100853\n",
            "resetting env. episode 875.000000, reward total was -19.000000. running mean: -20.089845\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.088946\n",
            "resetting env. episode 877.000000, reward total was -19.000000. running mean: -20.078057\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.087276\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.086403\n",
            "resetting env. episode 880.000000, reward total was -20.000000. running mean: -20.085539\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.094684\n",
            "resetting env. episode 882.000000, reward total was -21.000000. running mean: -20.103737\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.112700\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.121573\n",
            "resetting env. episode 885.000000, reward total was -20.000000. running mean: -20.120357\n",
            "resetting env. episode 886.000000, reward total was -21.000000. running mean: -20.129153\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.137862\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.146483\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.155018\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.163468\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -20.151834\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -20.140315\n",
            "resetting env. episode 893.000000, reward total was -19.000000. running mean: -20.128912\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.127623\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.126347\n",
            "resetting env. episode 896.000000, reward total was -19.000000. running mean: -20.115083\n",
            "resetting env. episode 897.000000, reward total was -19.000000. running mean: -20.103932\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.112893\n",
            "resetting env. episode 899.000000, reward total was -18.000000. running mean: -20.091764\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.100847\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.099838\n",
            "resetting env. episode 902.000000, reward total was -21.000000. running mean: -20.108840\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.107751\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.106674\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.115607\n",
            "resetting env. episode 906.000000, reward total was -20.000000. running mean: -20.114451\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.123306\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -20.122073\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.120853\n",
            "resetting env. episode 910.000000, reward total was -19.000000. running mean: -20.109644\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.108548\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.117462\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -20.116288\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.115125\n",
            "resetting env. episode 915.000000, reward total was -19.000000. running mean: -20.103973\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.102934\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.111904\n",
            "resetting env. episode 918.000000, reward total was -19.000000. running mean: -20.100785\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -20.089777\n",
            "resetting env. episode 920.000000, reward total was -20.000000. running mean: -20.088880\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.097991\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.097011\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.106041\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.114980\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.123831\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.132592\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.141266\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -20.139854\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.148455\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.156971\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.155401\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -20.143847\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.132409\n",
            "resetting env. episode 934.000000, reward total was -18.000000. running mean: -20.111084\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.119974\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.128774\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.127486\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.136211\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.144849\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.143401\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.151967\n",
            "resetting env. episode 942.000000, reward total was -19.000000. running mean: -20.140447\n",
            "resetting env. episode 943.000000, reward total was -20.000000. running mean: -20.139042\n",
            "resetting env. episode 944.000000, reward total was -19.000000. running mean: -20.127652\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -20.126376\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.135112\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.133761\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.132423\n",
            "resetting env. episode 949.000000, reward total was -19.000000. running mean: -20.121099\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.129888\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.138589\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.137203\n",
            "resetting env. episode 953.000000, reward total was -18.000000. running mean: -20.115831\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.114673\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.123526\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.122291\n",
            "resetting env. episode 957.000000, reward total was -18.000000. running mean: -20.101068\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.100057\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.109057\n",
            "resetting env. episode 960.000000, reward total was -18.000000. running mean: -20.087966\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.087086\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.096216\n",
            "resetting env. episode 963.000000, reward total was -19.000000. running mean: -20.085253\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -20.074401\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.063657\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.063020\n",
            "resetting env. episode 967.000000, reward total was -19.000000. running mean: -20.052390\n",
            "resetting env. episode 968.000000, reward total was -18.000000. running mean: -20.031866\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.041547\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.041132\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.040721\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.050313\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.059810\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.069212\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -20.068520\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.077835\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.087057\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -20.086186\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.095324\n",
            "resetting env. episode 980.000000, reward total was -19.000000. running mean: -20.084371\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.093527\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -20.092592\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.101666\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.110649\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.119543\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.128347\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -20.117064\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.125893\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.124634\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.133388\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.142054\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.140634\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.149227\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.157735\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.166158\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.164496\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.172851\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.181123\n",
            "resetting env. episode 999.000000, reward total was -18.000000. running mean: -20.159311\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.157718\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -20.146141\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.144680\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.153233\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.141701\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.140284\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.138881\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.147492\n",
            "resetting env. episode 1008.000000, reward total was -19.000000. running mean: -20.136017\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.144657\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.143210\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.151778\n",
            "resetting env. episode 1012.000000, reward total was -19.000000. running mean: -20.140260\n",
            "resetting env. episode 1013.000000, reward total was -19.000000. running mean: -20.128858\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -20.127569\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.126293\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.125031\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.123780\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.122542\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.121317\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.130104\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.128803\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.137515\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.136140\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.144778\n",
            "resetting env. episode 1025.000000, reward total was -20.000000. running mean: -20.143330\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.141897\n",
            "resetting env. episode 1027.000000, reward total was -18.000000. running mean: -20.120478\n",
            "resetting env. episode 1028.000000, reward total was -19.000000. running mean: -20.109273\n",
            "resetting env. episode 1029.000000, reward total was -18.000000. running mean: -20.088181\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.087299\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.096426\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.095462\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.094507\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.103562\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -20.102526\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.111501\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.120386\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -20.119182\n",
            "resetting env. episode 1039.000000, reward total was -18.000000. running mean: -20.097990\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -20.097010\n",
            "resetting env. episode 1041.000000, reward total was -20.000000. running mean: -20.096040\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.105080\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.114029\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.112889\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.121760\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.130542\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.129237\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.137945\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.136565\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.145199\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.153747\n",
            "resetting env. episode 1052.000000, reward total was -18.000000. running mean: -20.132210\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.140888\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.149479\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.147984\n",
            "resetting env. episode 1056.000000, reward total was -18.000000. running mean: -20.126504\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.135239\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -20.133887\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.142548\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -20.141123\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.149711\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.148214\n",
            "resetting env. episode 1063.000000, reward total was -18.000000. running mean: -20.126732\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.135465\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -20.134110\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.132769\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.141441\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.150027\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.148527\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.147041\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.155571\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.164015\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.172375\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -20.170651\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.178945\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.187155\n",
            "resetting env. episode 1077.000000, reward total was -19.000000. running mean: -20.175284\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.173531\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.181796\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.179978\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.188178\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -20.186296\n",
            "resetting env. episode 1083.000000, reward total was -18.000000. running mean: -20.164433\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.172789\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.181061\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.189250\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.197358\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -20.195384\n",
            "resetting env. episode 1089.000000, reward total was -18.000000. running mean: -20.173430\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.181696\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.189879\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.177980\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.176201\n",
            "resetting env. episode 1094.000000, reward total was -19.000000. running mean: -20.164439\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.172794\n",
            "resetting env. episode 1096.000000, reward total was -19.000000. running mean: -20.161066\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.169456\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.167761\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.176083\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.174323\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.182579\n",
            "resetting env. episode 1102.000000, reward total was -18.000000. running mean: -20.160754\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.169146\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.167455\n",
            "resetting env. episode 1105.000000, reward total was -19.000000. running mean: -20.155780\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.164222\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.172580\n",
            "resetting env. episode 1108.000000, reward total was -18.000000. running mean: -20.150854\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.159346\n",
            "resetting env. episode 1110.000000, reward total was -20.000000. running mean: -20.157752\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.156175\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.154613\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.153067\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.151536\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.160021\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.168421\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.176736\n",
            "resetting env. episode 1118.000000, reward total was -20.000000. running mean: -20.174969\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.173219\n",
            "resetting env. episode 1120.000000, reward total was -18.000000. running mean: -20.151487\n",
            "resetting env. episode 1121.000000, reward total was -19.000000. running mean: -20.139972\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -20.138573\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.147187\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.155715\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.164158\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.162516\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.170891\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.179182\n",
            "resetting env. episode 1129.000000, reward total was -16.000000. running mean: -20.137390\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -20.126016\n",
            "resetting env. episode 1131.000000, reward total was -19.000000. running mean: -20.114756\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.123609\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.132373\n",
            "resetting env. episode 1134.000000, reward total was -19.000000. running mean: -20.121049\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.119838\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -20.108640\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.117554\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.116378\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.125214\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -20.123962\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.122723\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.131495\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.140180\n",
            "resetting env. episode 1144.000000, reward total was -18.000000. running mean: -20.118779\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.117591\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.116415\n",
            "resetting env. episode 1147.000000, reward total was -18.000000. running mean: -20.095251\n",
            "resetting env. episode 1148.000000, reward total was -18.000000. running mean: -20.074298\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.083555\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.082720\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.091892\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.100974\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.099964\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.108964\n",
            "resetting env. episode 1155.000000, reward total was -18.000000. running mean: -20.087875\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -20.086996\n",
            "resetting env. episode 1157.000000, reward total was -19.000000. running mean: -20.076126\n",
            "resetting env. episode 1158.000000, reward total was -20.000000. running mean: -20.075365\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -20.084611\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.083765\n",
            "resetting env. episode 1161.000000, reward total was -18.000000. running mean: -20.062927\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.072298\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.081575\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -20.090759\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.089852\n",
            "resetting env. episode 1166.000000, reward total was -20.000000. running mean: -20.088953\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.098064\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -20.107083\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.116012\n",
            "resetting env. episode 1170.000000, reward total was -18.000000. running mean: -20.094852\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -20.093903\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.102964\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.101935\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.110915\n",
            "resetting env. episode 1175.000000, reward total was -19.000000. running mean: -20.099806\n",
            "resetting env. episode 1176.000000, reward total was -19.000000. running mean: -20.088808\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -20.087920\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -20.087041\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.096170\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.105209\n",
            "resetting env. episode 1181.000000, reward total was -20.000000. running mean: -20.104157\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.103115\n",
            "resetting env. episode 1183.000000, reward total was -18.000000. running mean: -20.082084\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.091263\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.090351\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.099447\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.108453\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -20.107368\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.116294\n",
            "resetting env. episode 1190.000000, reward total was -20.000000. running mean: -20.115131\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.123980\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.122740\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.131513\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.140198\n",
            "resetting env. episode 1195.000000, reward total was -20.000000. running mean: -20.138796\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.147408\n",
            "resetting env. episode 1197.000000, reward total was -19.000000. running mean: -20.135934\n",
            "resetting env. episode 1198.000000, reward total was -17.000000. running mean: -20.104574\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.113529\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.112393\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -20.111269\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.120157\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -20.118955\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -20.107766\n",
            "resetting env. episode 1205.000000, reward total was -18.000000. running mean: -20.086688\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.085821\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -20.084963\n",
            "resetting env. episode 1208.000000, reward total was -19.000000. running mean: -20.074113\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.083372\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -20.082538\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -20.071713\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.080996\n",
            "resetting env. episode 1213.000000, reward total was -20.000000. running mean: -20.080186\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.089384\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.098490\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -20.087505\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.086630\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.095764\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.104806\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.113758\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.122621\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -20.111394\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -20.110281\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.119178\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.117986\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.126806\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.135538\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -20.124183\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.132941\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.141611\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.140195\n",
            "resetting env. episode 1232.000000, reward total was -20.000000. running mean: -20.138793\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.137405\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -20.146031\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.154571\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -20.153025\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.161495\n",
            "resetting env. episode 1238.000000, reward total was -19.000000. running mean: -20.149880\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -20.138381\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.146998\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.155528\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -20.143972\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.152533\n",
            "resetting env. episode 1244.000000, reward total was -17.000000. running mean: -20.121007\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.129797\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.138499\n",
            "resetting env. episode 1247.000000, reward total was -20.000000. running mean: -20.137114\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.145743\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.154286\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -20.142743\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.141315\n",
            "resetting env. episode 1252.000000, reward total was -19.000000. running mean: -20.129902\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.128603\n",
            "resetting env. episode 1254.000000, reward total was -18.000000. running mean: -20.107317\n",
            "resetting env. episode 1255.000000, reward total was -18.000000. running mean: -20.086244\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.085381\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.084528\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.093682\n",
            "resetting env. episode 1259.000000, reward total was -19.000000. running mean: -20.082746\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.091918\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.100999\n",
            "resetting env. episode 1262.000000, reward total was -19.000000. running mean: -20.089989\n",
            "resetting env. episode 1263.000000, reward total was -20.000000. running mean: -20.089089\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.098198\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.097216\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.106244\n",
            "resetting env. episode 1267.000000, reward total was -19.000000. running mean: -20.095182\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.104230\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.103187\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.112156\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.121034\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.129824\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.138525\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -20.137140\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.135769\n",
            "resetting env. episode 1276.000000, reward total was -18.000000. running mean: -20.114411\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.123267\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -20.122034\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.120814\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.119606\n",
            "resetting env. episode 1281.000000, reward total was -20.000000. running mean: -20.118410\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.127226\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -20.125953\n",
            "resetting env. episode 1284.000000, reward total was -19.000000. running mean: -20.114694\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.113547\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.122412\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -20.121187\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.119976\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.128776\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.137488\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.146113\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.154652\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.163105\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.171474\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.179760\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.187962\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.196082\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -20.184122\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.192280\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -20.190358\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.198454\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.206469\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.204405\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.202361\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.210337\n",
            "resetting env. episode 1306.000000, reward total was -18.000000. running mean: -20.188234\n",
            "resetting env. episode 1307.000000, reward total was -20.000000. running mean: -20.186351\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.194488\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.202543\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.210518\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.218412\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -20.216228\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.224066\n",
            "resetting env. episode 1314.000000, reward total was -18.000000. running mean: -20.201825\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.199807\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.197809\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.205831\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.213773\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.211635\n",
            "resetting env. episode 1320.000000, reward total was -17.000000. running mean: -20.179519\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -20.167723\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.176046\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.174286\n",
            "resetting env. episode 1324.000000, reward total was -18.000000. running mean: -20.152543\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.151017\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.159507\n",
            "resetting env. episode 1327.000000, reward total was -20.000000. running mean: -20.157912\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.166333\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.174670\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.182923\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -20.171094\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.179383\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.187589\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.195713\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -20.193756\n",
            "resetting env. episode 1336.000000, reward total was -19.000000. running mean: -20.181818\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -20.180000\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.188200\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.186318\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -20.184455\n",
            "resetting env. episode 1341.000000, reward total was -19.000000. running mean: -20.172611\n",
            "resetting env. episode 1342.000000, reward total was -19.000000. running mean: -20.160884\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.169276\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.167583\n",
            "resetting env. episode 1345.000000, reward total was -19.000000. running mean: -20.155907\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.164348\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -20.162704\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.161077\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.169467\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -20.157772\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.166194\n",
            "resetting env. episode 1352.000000, reward total was -17.000000. running mean: -20.134532\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -20.123187\n",
            "resetting env. episode 1354.000000, reward total was -19.000000. running mean: -20.111955\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.110836\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -20.109727\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.118630\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.127444\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -20.116169\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.125007\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.123757\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.132520\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.141195\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.149783\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -20.138285\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -20.136902\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.145533\n",
            "resetting env. episode 1368.000000, reward total was -19.000000. running mean: -20.134078\n",
            "resetting env. episode 1369.000000, reward total was -17.000000. running mean: -20.102737\n",
            "resetting env. episode 1370.000000, reward total was -18.000000. running mean: -20.081710\n",
            "resetting env. episode 1371.000000, reward total was -18.000000. running mean: -20.060892\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.060283\n",
            "resetting env. episode 1373.000000, reward total was -19.000000. running mean: -20.049681\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.059184\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.068592\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.067906\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.077227\n",
            "resetting env. episode 1378.000000, reward total was -19.000000. running mean: -20.066455\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -20.055790\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -20.055232\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.044680\n",
            "resetting env. episode 1382.000000, reward total was -19.000000. running mean: -20.034233\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -20.023891\n",
            "resetting env. episode 1384.000000, reward total was -18.000000. running mean: -20.003652\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.013615\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.023479\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.033244\n",
            "resetting env. episode 1388.000000, reward total was -20.000000. running mean: -20.032912\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.042583\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -20.042157\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -20.041736\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -20.041318\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.050905\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.060396\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.059792\n",
            "resetting env. episode 1396.000000, reward total was -19.000000. running mean: -20.049194\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.058702\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -20.048115\n",
            "resetting env. episode 1399.000000, reward total was -20.000000. running mean: -20.047634\n",
            "resetting env. episode 1400.000000, reward total was -20.000000. running mean: -20.047158\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.046686\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.056219\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.065657\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.075000\n",
            "resetting env. episode 1405.000000, reward total was -19.000000. running mean: -20.064250\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.073608\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.082872\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.092043\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.101123\n",
            "resetting env. episode 1410.000000, reward total was -19.000000. running mean: -20.090111\n",
            "resetting env. episode 1411.000000, reward total was -19.000000. running mean: -20.079210\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.088418\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.097534\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -20.086559\n",
            "resetting env. episode 1415.000000, reward total was -19.000000. running mean: -20.075693\n",
            "resetting env. episode 1416.000000, reward total was -19.000000. running mean: -20.064936\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.074287\n",
            "resetting env. episode 1418.000000, reward total was -19.000000. running mean: -20.063544\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.062909\n",
            "resetting env. episode 1420.000000, reward total was -19.000000. running mean: -20.052279\n",
            "resetting env. episode 1421.000000, reward total was -18.000000. running mean: -20.031757\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.041439\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -20.041025\n",
            "resetting env. episode 1424.000000, reward total was -18.000000. running mean: -20.020614\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.030408\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -20.020104\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.029903\n",
            "resetting env. episode 1428.000000, reward total was -20.000000. running mean: -20.029604\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.029308\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.029015\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.028725\n",
            "resetting env. episode 1432.000000, reward total was -18.000000. running mean: -20.008438\n",
            "resetting env. episode 1433.000000, reward total was -18.000000. running mean: -19.988353\n",
            "resetting env. episode 1434.000000, reward total was -19.000000. running mean: -19.978470\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -19.988685\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -19.998798\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.008810\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.018722\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.028535\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.038250\n",
            "resetting env. episode 1441.000000, reward total was -16.000000. running mean: -19.997867\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -19.997888\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.007909\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.017830\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.027652\n",
            "resetting env. episode 1446.000000, reward total was -18.000000. running mean: -20.007376\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -20.007302\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -20.007229\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.007156\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.017085\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -20.006914\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.016845\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.026676\n",
            "resetting env. episode 1454.000000, reward total was -19.000000. running mean: -20.016410\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -20.016246\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.026083\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -20.025822\n",
            "resetting env. episode 1458.000000, reward total was -19.000000. running mean: -20.015564\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.015408\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.025254\n",
            "resetting env. episode 1461.000000, reward total was -19.000000. running mean: -20.015002\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.024852\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.024603\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.034357\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.044014\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.053574\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.063038\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.072407\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -20.071683\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -20.070967\n",
            "resetting env. episode 1471.000000, reward total was -19.000000. running mean: -20.060257\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -20.059654\n",
            "resetting env. episode 1473.000000, reward total was -18.000000. running mean: -20.039058\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.038667\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.048281\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.057798\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.067220\n",
            "resetting env. episode 1478.000000, reward total was -19.000000. running mean: -20.056548\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -20.045982\n",
            "resetting env. episode 1480.000000, reward total was -19.000000. running mean: -20.035522\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -20.035167\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -20.034815\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.044467\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.054023\n",
            "resetting env. episode 1485.000000, reward total was -18.000000. running mean: -20.033482\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.043147\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.052716\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -20.052189\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.051667\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -20.051150\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.060639\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -20.060032\n",
            "resetting env. episode 1493.000000, reward total was -17.000000. running mean: -20.029432\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.039138\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.048746\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.058259\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -20.057676\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.067100\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.076429\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.085664\n",
            "CPU times: user 1h 19min 55s, sys: 37min 27s, total: 1h 57min 23s\n",
            "Wall time: 1h 43s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "3abb82eb-e32c-4f15-c083-90c228c8bc46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}