{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ajay800.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "a099bfeb-f858-4e69-a0b5-ceddbb6e73cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 14.5 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=5e454aa529fe5d43a10f2686c7c445f6262824f0476a883049a805fc99a2fa23\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "6794372d-6ec6-4ae2-cac2-377b5427aad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "7485a4c6-dbb3-486a-e51f-63b154198fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "2002ac6e-20de-4267-891f-889fcdfe67e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "05127fe6-7153-444a-9b83-32e5e3100192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 800 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "65abd0eb-3a76-498c-98fb-9fef3e8bd0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.980100\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.980299\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980496\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.970691\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.970984\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.971274\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.971562\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.971846\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.952127\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.942606\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.933180\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.923848\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.924610\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.915364\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.916210\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.907048\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.907978\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.898898\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.899909\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.900910\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.901901\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.892882\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.893953\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.895013\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.886063\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.867202\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.858530\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.859945\n",
            "resetting env. episode 33.000000, reward total was -17.000000. running mean: -20.821346\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.813132\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.815001\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.816851\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.818682\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.800496\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.802491\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.794466\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.796521\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.798556\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.800570\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.792565\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.794639\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.796693\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.778726\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.780938\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.783129\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.785298\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.787445\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.789570\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.781675\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.783858\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.786019\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.778159\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.780377\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -20.762574\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.764948\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.767298\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.769625\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.771929\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.754210\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.756668\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.749101\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.751610\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.754094\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.756553\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.758988\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.761398\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.763784\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.766146\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.768484\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.760800\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.763192\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.755560\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.758004\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.760424\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.752820\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.745292\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.747839\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.740360\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.742957\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.745527\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.748072\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.750591\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.743085\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.745654\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.748198\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.750716\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.733209\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.735877\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.738518\n",
            "resetting env. episode 94.000000, reward total was -18.000000. running mean: -20.711133\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.704021\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.706981\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.709911\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.702812\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.695784\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.698826\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.701838\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.704820\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.707771\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.700694\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.703687\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.696650\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.689683\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.692787\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.695859\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.698900\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.701911\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.704892\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.707843\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.700765\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.703757\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.696719\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.689752\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.682855\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.686026\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.689166\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.682274\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.685451\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -20.668597\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.661911\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.645292\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.648839\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.632351\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.636027\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.629667\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.623370\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.627136\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.630865\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.634556\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.628211\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.631929\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.625609\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.619353\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.623160\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.606928\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.610859\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.614750\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.608603\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.592517\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.596592\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.600626\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.584619\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.578773\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.582986\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.587156\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.571284\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.565571\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.549916\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.544416\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.538972\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.543583\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.538147\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.542765\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.547338\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.551864\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.556346\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.560782\n",
            "resetting env. episode 162.000000, reward total was -18.000000. running mean: -20.535174\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.519823\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.524624\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.529378\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.534084\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.528743\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.523456\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.528221\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.532939\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.537610\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.542234\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.536811\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.541443\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.546029\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.550569\n",
            "resetting env. episode 177.000000, reward total was -18.000000. running mean: -20.525063\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.509812\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.514714\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.519567\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.514371\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.519228\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.524035\n",
            "resetting env. episode 184.000000, reward total was -19.000000. running mean: -20.508795\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.513707\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.518570\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.503384\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.498350\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.483367\n",
            "resetting env. episode 190.000000, reward total was -18.000000. running mean: -20.458533\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.453948\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.459408\n",
            "resetting env. episode 193.000000, reward total was -18.000000. running mean: -20.434814\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.440466\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.446062\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.451601\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.457085\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.442514\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.448089\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.453608\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.459072\n",
            "resetting env. episode 202.000000, reward total was -18.000000. running mean: -20.434481\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.430136\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.435835\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.441477\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.447062\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.452591\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.458065\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.443485\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.439050\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.444659\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.450213\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.435711\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.441354\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.436940\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.442571\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.448145\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.453664\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.449127\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.454636\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.450089\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.445588\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.441132\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.446721\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.452254\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.447731\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.453254\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.448722\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.454234\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.439692\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.445295\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.450842\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.456334\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.461770\n",
            "resetting env. episode 235.000000, reward total was -17.000000. running mean: -20.427153\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.412881\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.418752\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.424565\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.420319\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.426116\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.431855\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.417536\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.423361\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.409127\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.395036\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.401086\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.407075\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.403004\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.408974\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.404884\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.400835\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.406827\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.412759\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.408631\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.414545\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.420399\n",
            "resetting env. episode 257.000000, reward total was -18.000000. running mean: -20.396195\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.402233\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.388211\n",
            "resetting env. episode 260.000000, reward total was -18.000000. running mean: -20.364329\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.370686\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.376979\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.383209\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.389377\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.395483\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.401528\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.407513\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.393438\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.389504\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.395609\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.381652\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.387836\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.393958\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.390018\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.396118\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.392157\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.388235\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.394353\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.390409\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.386505\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.382640\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.388814\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.384926\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.381076\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.387266\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.383393\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.389559\n",
            "resetting env. episode 288.000000, reward total was -18.000000. running mean: -20.365663\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.362007\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.348387\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.344903\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.341454\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.328039\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.324759\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.331511\n",
            "resetting env. episode 296.000000, reward total was -18.000000. running mean: -20.308196\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.315114\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.311963\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.318843\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.325655\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.332398\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.339074\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.345684\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.352227\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.348705\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.355218\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.351665\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.348149\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.344667\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.341221\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.347808\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.344330\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.350887\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.337378\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.344004\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.350564\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.347059\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.353588\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.350052\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.356552\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.342986\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.349556\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.336061\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.342700\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.349273\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.335780\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.342423\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.348998\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.355508\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.361953\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.358334\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.364750\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.361103\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.357492\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.363917\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.370278\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.376575\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.382809\n",
            "resetting env. episode 339.000000, reward total was -18.000000. running mean: -20.358981\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.355391\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.351837\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.358319\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.354736\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.341188\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.347777\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.344299\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.340856\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.347447\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.353973\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.360433\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.356829\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.353260\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.349728\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.356231\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.352668\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.349142\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.355650\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.352094\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.338573\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.335187\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.331835\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.338517\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.325132\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.331880\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.338562\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.335176\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.341824\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.348406\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.354922\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.351373\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.357859\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.354280\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.360738\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.367130\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.363459\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.369824\n",
            "resetting env. episode 377.000000, reward total was -18.000000. running mean: -20.346126\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.352665\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.339138\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.345747\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.342289\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.338866\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.335478\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.342123\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.348702\n",
            "resetting env. episode 386.000000, reward total was -18.000000. running mean: -20.325215\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.331963\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.338643\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.325256\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.312004\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.308884\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.305795\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.302737\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.309710\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.296613\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.303646\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.310610\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.307504\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.314429\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.311285\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.318172\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.324990\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.331740\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.338423\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.325038\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.331788\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.318470\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.325286\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.322033\n",
            "resetting env. episode 410.000000, reward total was -19.000000. running mean: -20.308812\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.315724\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.302567\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.309541\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.316446\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.323281\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.310049\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.306948\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.313879\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.300740\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.307732\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.314655\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.321509\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.318294\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.315111\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.321959\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.318740\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.325552\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.322297\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.319074\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.315883\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.312724\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.309597\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.296501\n",
            "resetting env. episode 434.000000, reward total was -19.000000. running mean: -20.283536\n",
            "resetting env. episode 435.000000, reward total was -19.000000. running mean: -20.270701\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.267994\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.265314\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.252661\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.260134\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.267533\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.274857\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.282109\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.289288\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.286395\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.283531\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.290696\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.297789\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.294811\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.281863\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.289044\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.286154\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.293292\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.300359\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.287356\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.284482\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.291637\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.298721\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.295734\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.292776\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.299849\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.296850\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.303882\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.310843\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.307734\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.314657\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.321510\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.328295\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.325012\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.321762\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.318545\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.315359\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.312206\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.309083\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.315993\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.322833\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.319604\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.326408\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.333144\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.339813\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.346415\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.332951\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.339621\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.326225\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.332963\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.329633\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.326337\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.333073\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.339743\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.346345\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.352882\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.349353\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.335859\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.342501\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.349076\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.355585\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.362029\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.368409\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.364725\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.361077\n",
            "resetting env. episode 500.000000, reward total was -16.000000. running mean: -20.317467\n",
            "CPU times: user 50min 58s, sys: 10min 25s, total: 1h 1min 23s\n",
            "Wall time: 31min 20s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "4d35c06a-3bfa-4aa2-cd52-87ec00ed0e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -18.000000. running mean: -18.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -18.030000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -18.059700\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -18.079103\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -18.088312\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -18.117429\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -18.136255\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -18.164892\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -18.183243\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -18.191411\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -18.219497\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -18.247302\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -18.274829\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -18.302080\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -18.319059\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -18.345869\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -18.362410\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -18.388786\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -18.414898\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -18.420749\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -18.436542\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -18.462176\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -18.487555\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -18.492679\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -18.497752\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -18.512775\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -18.527647\n",
            "resetting env. episode 28.000000, reward total was -18.000000. running mean: -18.522371\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -18.537147\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -18.561775\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -18.586158\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -18.580296\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -18.604493\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -18.628448\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -18.632164\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -18.655842\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -18.679284\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -18.682491\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -18.685666\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -18.708809\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -18.721721\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -18.724504\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -18.737259\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -18.749886\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -18.762387\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -18.774764\n",
            "resetting env. episode 47.000000, reward total was -18.000000. running mean: -18.767016\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -18.789346\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -18.801452\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -18.823438\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -18.845203\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -18.846751\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -18.848284\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -18.869801\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -18.871103\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -18.882392\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -18.903568\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -18.924532\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -18.945287\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -18.965834\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -18.986176\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.006314\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -19.006251\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -19.006188\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.026127\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -19.035865\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.055507\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.074952\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -19.094202\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -19.103260\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.122227\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.141005\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -19.149595\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -19.148099\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.166618\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -19.164952\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.183302\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -19.191469\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.199555\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -19.197559\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.205584\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.213528\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -19.221392\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.239179\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.246787\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.254319\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -19.261776\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.279158\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.286366\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.303503\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -19.310468\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -19.317363\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.334189\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.350847\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -19.347339\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.353866\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.370327\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -19.386624\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -19.392757\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -19.398830\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.414842\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -19.430693\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -19.436386\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -19.452022\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -19.447502\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -19.443027\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.458597\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.474011\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.489271\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -19.494378\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -19.499434\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -19.504440\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.509396\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -19.514302\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.529159\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.543867\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.558428\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -19.562844\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -19.557216\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -19.551643\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -19.546127\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -19.550666\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.565159\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.569507\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -19.583812\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.597974\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -19.591995\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.606075\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -19.620014\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -19.613814\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.627676\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.641399\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.654985\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.668435\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.681751\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -19.684933\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.698084\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -19.701103\n",
            "resetting env. episode 139.000000, reward total was -18.000000. running mean: -19.684092\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -19.687251\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -19.700378\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -19.713375\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -19.726241\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -19.738979\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.751589\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -19.764073\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -19.766432\n",
            "resetting env. episode 148.000000, reward total was -19.000000. running mean: -19.758768\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.771180\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -19.763468\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -19.765834\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -19.778175\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -19.790394\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.792490\n",
            "resetting env. episode 155.000000, reward total was -19.000000. running mean: -19.784565\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -19.796719\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -19.808752\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.820664\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -19.832458\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.844133\n",
            "resetting env. episode 161.000000, reward total was -18.000000. running mean: -19.825692\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -19.837435\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.849061\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -19.860570\n",
            "resetting env. episode 165.000000, reward total was -17.000000. running mean: -19.831964\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -19.833645\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.845308\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -19.846855\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -19.848387\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.859903\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.871304\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -19.882591\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -19.893765\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -19.884827\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -19.895979\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -19.897019\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.908049\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -19.908968\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -19.909879\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -19.920780\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -19.931572\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -19.942256\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -19.932834\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -19.943505\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -19.944070\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -19.954630\n",
            "resetting env. episode 187.000000, reward total was -17.000000. running mean: -19.925083\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -19.925833\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -19.926574\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.937308\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -19.947935\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -19.948456\n",
            "resetting env. episode 193.000000, reward total was -18.000000. running mean: -19.928971\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -19.929682\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -19.940385\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -19.950981\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -19.961471\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -19.971857\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -19.972138\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -19.982417\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -19.992592\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -19.992667\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -19.982740\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -19.992912\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -19.982983\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -19.983154\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -19.993322\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.003389\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.003355\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.003321\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.013288\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.003155\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.013124\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.012992\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.022863\n",
            "resetting env. episode 216.000000, reward total was -18.000000. running mean: -20.002634\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.002608\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.012581\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.022456\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.032231\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.041909\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.051490\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.060975\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.060365\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.049761\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.039264\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.038871\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.048482\n",
            "resetting env. episode 229.000000, reward total was -19.000000. running mean: -20.037998\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.047618\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.047141\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.056670\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.066103\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.075442\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.084688\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.083841\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.093003\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.092073\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.081152\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.090340\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.079437\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.078643\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.087856\n",
            "resetting env. episode 244.000000, reward total was -19.000000. running mean: -20.076978\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.076208\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.085446\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.094591\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.103645\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.112609\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.121483\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.120268\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.119065\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.127875\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.126596\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.135330\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.143977\n",
            "resetting env. episode 257.000000, reward total was -17.000000. running mean: -20.112537\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.101412\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.110397\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.099293\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.108300\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -20.097217\n",
            "resetting env. episode 263.000000, reward total was -18.000000. running mean: -20.076245\n",
            "resetting env. episode 264.000000, reward total was -18.000000. running mean: -20.055483\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.064928\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.054279\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.053736\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.053199\n",
            "resetting env. episode 269.000000, reward total was -18.000000. running mean: -20.032667\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.042340\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.051917\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.041397\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.040983\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.040574\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.040168\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.039766\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.049368\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.048875\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.038386\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.038002\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.037622\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.047246\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.056773\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.056206\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.065644\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.074987\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.064237\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.053595\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.063059\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.052428\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.061904\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.071285\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.070572\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.079867\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.079068\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.088277\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.087394\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.096521\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.105555\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.094500\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.083555\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.092719\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.101792\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.110774\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.109666\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.118570\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.117384\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.126210\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.124948\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.113699\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.102562\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.111536\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.120421\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.129216\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.127924\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.116645\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.125479\n",
            "resetting env. episode 318.000000, reward total was -18.000000. running mean: -20.104224\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.113182\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.122050\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.130829\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.139521\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.138126\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.146744\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.145277\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.143824\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.152386\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.140862\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.139454\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.128059\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.126778\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.135511\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.144156\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.152714\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.161187\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.169575\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.177879\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.166100\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.174439\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.182695\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -20.160868\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.169259\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.167567\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.175891\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.184132\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.192291\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.180368\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.178564\n",
            "resetting env. episode 349.000000, reward total was -18.000000. running mean: -20.156779\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.155211\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.143659\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.152222\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.150700\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.149193\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.157701\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.166124\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.174463\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.172718\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.180991\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.169181\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.177489\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.175714\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.173957\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.172218\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.170495\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.178790\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.177003\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.175233\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.173480\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.171745\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.170028\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.168328\n",
            "resetting env. episode 373.000000, reward total was -18.000000. running mean: -20.146644\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.145178\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.153726\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.162189\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.160567\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.168961\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.177272\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.185499\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.193644\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.181708\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.189891\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.187992\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.196112\n",
            "resetting env. episode 386.000000, reward total was -18.000000. running mean: -20.174151\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.172409\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.160685\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.149078\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.157587\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.146011\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.134551\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.133206\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.141874\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.130455\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.129151\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.117859\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.126680\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.135414\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.134059\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.142719\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.141292\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.149879\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.158380\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.156796\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.155228\n",
            "resetting env. episode 407.000000, reward total was -18.000000. running mean: -20.133676\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.142339\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.140916\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.139507\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.138112\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.136730\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.145363\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.143910\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.132470\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.131146\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.139834\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.138436\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.147052\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.145581\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.144125\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.152684\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.161157\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.169546\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.167850\n",
            "resetting env. episode 426.000000, reward total was -19.000000. running mean: -20.156172\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.144610\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -20.123164\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.131932\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.140613\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.149207\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.157715\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.166138\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.164476\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.172831\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.181103\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.189292\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.197399\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.185425\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.183571\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.191735\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.199818\n",
            "resetting env. episode 443.000000, reward total was -18.000000. running mean: -20.177820\n",
            "resetting env. episode 444.000000, reward total was -17.000000. running mean: -20.146041\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.154581\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.163035\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.171405\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.169691\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.177994\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.166214\n",
            "resetting env. episode 451.000000, reward total was -17.000000. running mean: -20.134552\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.123206\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.121974\n",
            "resetting env. episode 454.000000, reward total was -18.000000. running mean: -20.100754\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.099747\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.088749\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.097862\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.096883\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.085915\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.085055\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.084205\n",
            "resetting env. episode 462.000000, reward total was -17.000000. running mean: -20.053363\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.052829\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.062301\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.051678\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.061161\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.060549\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.069944\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.079245\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.078452\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.077668\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.086891\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.076022\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.075262\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.074509\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.063764\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.063126\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.062495\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.061870\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.071251\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.070539\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.059834\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.069235\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.058543\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.047957\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -20.027478\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.017203\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.027031\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.036761\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.036393\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.046029\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.055569\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.045013\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.044563\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.044117\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.033676\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.033340\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.043006\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.042576\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.042150\n",
            "CPU times: user 54min 7s, sys: 10min 54s, total: 1h 5min 1s\n",
            "Wall time: 33min 3s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "6eca9998-44ae-43c2-bc9c-33f3eba33513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "0b42ca16-5781-4d78-c000-a7947de2841b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.009801\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.009703\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.019606\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.019410\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.029216\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.038924\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.048534\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -20.038049\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.047669\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.057192\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.046620\n",
            "resetting env. episode 16.000000, reward total was -18.000000. running mean: -20.026154\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.035892\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.035533\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.045178\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.054726\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.064179\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.073537\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.082802\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.081974\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.071154\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.080442\n",
            "resetting env. episode 27.000000, reward total was -17.000000. running mean: -20.049638\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.049142\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.058650\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.068064\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.077383\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.086609\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.085743\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.084886\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.094037\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.103097\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.112066\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.120945\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.119735\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.118538\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.117353\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -20.106179\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.115117\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.113966\n",
            "resetting env. episode 45.000000, reward total was -16.000000. running mean: -20.072827\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.072098\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.061377\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.060764\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.060156\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.069554\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.068859\n",
            "resetting env. episode 52.000000, reward total was -19.000000. running mean: -20.058170\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.067589\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.066913\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.066244\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.065581\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.074925\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.084176\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.083334\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.092501\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.091576\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.090660\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.099754\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.108756\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.107668\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.106592\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.115526\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.114371\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.113227\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.122095\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.120874\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.129665\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.118368\n",
            "resetting env. episode 74.000000, reward total was -18.000000. running mean: -20.097185\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.096213\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.105251\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.114198\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -20.093056\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.092126\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.091204\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.100292\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.109289\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.118196\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.127014\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.125744\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.134487\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.123142\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.131911\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.140592\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.149186\n",
            "resetting env. episode 91.000000, reward total was -18.000000. running mean: -20.127694\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.126417\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.135153\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.143801\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -20.132363\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.131039\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.139729\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.138332\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.136948\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.125579\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.114323\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.103180\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.102148\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.101127\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.110115\n",
            "resetting env. episode 106.000000, reward total was -17.000000. running mean: -20.079014\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.078224\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.077442\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.086667\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.095801\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.084843\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.083994\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.093154\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.082223\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.081401\n",
            "resetting env. episode 116.000000, reward total was -17.000000. running mean: -20.050587\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.050081\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.059580\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.068984\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.068294\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.077611\n",
            "resetting env. episode 122.000000, reward total was -18.000000. running mean: -20.056835\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.066267\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.065604\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.064948\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.064299\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.063656\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.063019\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.062389\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.071765\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.071047\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.080337\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.089534\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.098638\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.107652\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.116575\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.115410\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.114255\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.113113\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.121982\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.130762\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.129454\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.118160\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.126978\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.135708\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.144351\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.152908\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.151379\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.159865\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.168266\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.176584\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.184818\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.192970\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.191040\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.189130\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.197238\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.195266\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.203313\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.201280\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.209267\n",
            "resetting env. episode 161.000000, reward total was -17.000000. running mean: -20.177175\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.175403\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.183649\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.181812\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -20.159994\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.168394\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.176710\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.184943\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.183094\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.191263\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.199350\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.207357\n",
            "resetting env. episode 173.000000, reward total was -18.000000. running mean: -20.185283\n",
            "resetting env. episode 174.000000, reward total was -18.000000. running mean: -20.163430\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.171796\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.160078\n",
            "resetting env. episode 177.000000, reward total was -19.000000. running mean: -20.148477\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.136993\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.125623\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.124366\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.133123\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.141791\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.150374\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.148870\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.157381\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -20.145807\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.154349\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.152806\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.141278\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.139865\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.148466\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.146982\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.135512\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.144157\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.152715\n",
            "resetting env. episode 196.000000, reward total was -18.000000. running mean: -20.131188\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.139876\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.148477\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.156993\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.155423\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.163868\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.172230\n",
            "resetting env. episode 203.000000, reward total was -18.000000. running mean: -20.150507\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.159002\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.157412\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.155838\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.154280\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.142737\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.151310\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.159797\n",
            "resetting env. episode 211.000000, reward total was -17.000000. running mean: -20.128199\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.136917\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.145547\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.154092\n",
            "resetting env. episode 215.000000, reward total was -18.000000. running mean: -20.132551\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.131226\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.129913\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.138614\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.147228\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.145756\n",
            "resetting env. episode 221.000000, reward total was -19.000000. running mean: -20.134298\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.142955\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.151526\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.160010\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.148410\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.156926\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.165357\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.173703\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.181966\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.190147\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.188245\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.196363\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.204399\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.202355\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.200332\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.188328\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.186445\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.184580\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.182735\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.190907\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.188998\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.187108\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.195237\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.203285\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.201252\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.189239\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.197347\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.195374\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.193420\n",
            "resetting env. episode 250.000000, reward total was -18.000000. running mean: -20.171486\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.179771\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.177973\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.186193\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.194331\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.192388\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.200464\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.208460\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.216375\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.224211\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.231969\n",
            "resetting env. episode 261.000000, reward total was -19.000000. running mean: -20.219649\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.227453\n",
            "resetting env. episode 263.000000, reward total was -19.000000. running mean: -20.215178\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.213027\n",
            "resetting env. episode 265.000000, reward total was -19.000000. running mean: -20.200896\n",
            "resetting env. episode 266.000000, reward total was -18.000000. running mean: -20.178887\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.177099\n",
            "resetting env. episode 268.000000, reward total was -17.000000. running mean: -20.145328\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.153874\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.162336\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.160712\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.169105\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.157414\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.145840\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.154381\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.162838\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.161209\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.159597\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.168001\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.166321\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.164658\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.173011\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.171281\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.179568\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.187773\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.185895\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.184036\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.192196\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.200274\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.198271\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.206288\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.204225\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.202183\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.200161\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.198160\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.206178\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.204116\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.202075\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.200054\n",
            "resetting env. episode 300.000000, reward total was -18.000000. running mean: -20.178054\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.176273\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.174511\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.182766\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.170938\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.169229\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.167536\n",
            "resetting env. episode 307.000000, reward total was -16.000000. running mean: -20.125861\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.124602\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.133356\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.142023\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.130602\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.129296\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.138003\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.146623\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.135157\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.133806\n",
            "resetting env. episode 317.000000, reward total was -17.000000. running mean: -20.102468\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.111443\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.110328\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.119225\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.118033\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.116853\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.105684\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.104627\n",
            "resetting env. episode 325.000000, reward total was -19.000000. running mean: -20.093581\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.082645\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.091819\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.090901\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.089991\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.089092\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.088201\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.087319\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.076445\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.065681\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.075024\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.064274\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.073631\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.062895\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.062266\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.071643\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.080927\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.090118\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.099216\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.108224\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.117142\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.095971\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.105011\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.113961\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.112821\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.111693\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.120576\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.129370\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.138077\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.136696\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.145329\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.143876\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.142437\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.141012\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.139602\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.138206\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.136824\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.135456\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.134101\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.132760\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.141433\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.140018\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.138618\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.137232\n",
            "resetting env. episode 369.000000, reward total was -18.000000. running mean: -20.115860\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.124701\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.133454\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.142120\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.130698\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.139391\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.137998\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.146618\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.155151\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.163600\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.171964\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.170244\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.168542\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.156856\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.165288\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.173635\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.161899\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.170280\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.178577\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.176791\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.175023\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.163273\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.151640\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.150124\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.148623\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.137136\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.125765\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.134507\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.143162\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.141731\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.150313\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.158810\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.147222\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.145750\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.144292\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.152849\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.161321\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.169708\n",
            "resetting env. episode 407.000000, reward total was -18.000000. running mean: -20.148011\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.136531\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.125165\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.133914\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.132574\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.131249\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.119936\n",
            "resetting env. episode 414.000000, reward total was -19.000000. running mean: -20.108737\n",
            "resetting env. episode 415.000000, reward total was -19.000000. running mean: -20.097649\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.096673\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.105706\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.114649\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.123503\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.122268\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.131045\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.139735\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.148337\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.156854\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.155285\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.153732\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.162195\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.160573\n",
            "resetting env. episode 429.000000, reward total was -18.000000. running mean: -20.138967\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.147578\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.156102\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.164541\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.152896\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.161367\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.159753\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.168155\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.176474\n",
            "resetting env. episode 438.000000, reward total was -19.000000. running mean: -20.164709\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.163062\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.161431\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.159817\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.168219\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.166537\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.154871\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.163323\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.151689\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.150173\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.158671\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.147084\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.135613\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.124257\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.113015\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.111884\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.110766\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.109658\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.118561\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.117376\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.126202\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.134940\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.143591\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.152155\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.150633\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.139127\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.137735\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.146358\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.144895\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.133446\n",
            "resetting env. episode 468.000000, reward total was -18.000000. running mean: -20.112111\n",
            "resetting env. episode 469.000000, reward total was -19.000000. running mean: -20.100990\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.109980\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.118880\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.127692\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.126415\n",
            "resetting env. episode 474.000000, reward total was -18.000000. running mean: -20.105150\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.104099\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.103058\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.112027\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.110907\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.109798\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.118700\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.107513\n",
            "resetting env. episode 482.000000, reward total was -19.000000. running mean: -20.096438\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.105474\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -20.084419\n",
            "resetting env. episode 485.000000, reward total was -18.000000. running mean: -20.063575\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.062939\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.062309\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.071686\n",
            "resetting env. episode 489.000000, reward total was -18.000000. running mean: -20.050970\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.050460\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.059955\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.059356\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.068762\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.078075\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.077294\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.076521\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.085756\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.094898\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.093949\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.083010\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.082179\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -20.081358\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.090544\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.099639\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.098642\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.107656\n",
            "resetting env. episode 507.000000, reward total was -19.000000. running mean: -20.096579\n",
            "resetting env. episode 508.000000, reward total was -19.000000. running mean: -20.085614\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.094757\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.103810\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.102772\n",
            "resetting env. episode 512.000000, reward total was -21.000000. running mean: -20.111744\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.120627\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.129420\n",
            "resetting env. episode 515.000000, reward total was -18.000000. running mean: -20.108126\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.107045\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.115974\n",
            "resetting env. episode 518.000000, reward total was -19.000000. running mean: -20.104815\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.103766\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.112729\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.111602\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.110486\n",
            "resetting env. episode 523.000000, reward total was -18.000000. running mean: -20.089381\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -20.078487\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.077702\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.086925\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.096056\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.105095\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.114044\n",
            "resetting env. episode 530.000000, reward total was -18.000000. running mean: -20.092904\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.101975\n",
            "resetting env. episode 532.000000, reward total was -19.000000. running mean: -20.090955\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.090045\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.099145\n",
            "resetting env. episode 535.000000, reward total was -18.000000. running mean: -20.078154\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -20.067372\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.076698\n",
            "resetting env. episode 538.000000, reward total was -18.000000. running mean: -20.055931\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.055372\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.064818\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.064170\n",
            "resetting env. episode 542.000000, reward total was -19.000000. running mean: -20.053528\n",
            "resetting env. episode 543.000000, reward total was -20.000000. running mean: -20.052993\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.062463\n",
            "resetting env. episode 545.000000, reward total was -19.000000. running mean: -20.051839\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.061320\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.070707\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.070000\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.069300\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.078607\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.087821\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.096943\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.105973\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.104913\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.113864\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.112726\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.121598\n",
            "resetting env. episode 558.000000, reward total was -21.000000. running mean: -20.130382\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.139079\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.147688\n",
            "resetting env. episode 561.000000, reward total was -21.000000. running mean: -20.156211\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.164649\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.173002\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.181272\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.189460\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.197565\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -20.195589\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.203633\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -20.191597\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.199681\n",
            "resetting env. episode 571.000000, reward total was -20.000000. running mean: -20.197684\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.195707\n",
            "resetting env. episode 573.000000, reward total was -18.000000. running mean: -20.173750\n",
            "resetting env. episode 574.000000, reward total was -19.000000. running mean: -20.162013\n",
            "resetting env. episode 575.000000, reward total was -20.000000. running mean: -20.160393\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.158789\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.157201\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.165629\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.163973\n",
            "resetting env. episode 580.000000, reward total was -19.000000. running mean: -20.152333\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.160810\n",
            "resetting env. episode 582.000000, reward total was -19.000000. running mean: -20.149202\n",
            "resetting env. episode 583.000000, reward total was -19.000000. running mean: -20.137709\n",
            "resetting env. episode 584.000000, reward total was -19.000000. running mean: -20.126332\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -20.125069\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.123818\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.132580\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.121254\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.130042\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.138741\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.147354\n",
            "resetting env. episode 592.000000, reward total was -18.000000. running mean: -20.125880\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.134622\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.143275\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.151843\n",
            "resetting env. episode 596.000000, reward total was -19.000000. running mean: -20.140324\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.148921\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.147432\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.155958\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.164398\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.162754\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.171126\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.179415\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.187621\n",
            "resetting env. episode 605.000000, reward total was -18.000000. running mean: -20.165745\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.174087\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.172346\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.180623\n",
            "resetting env. episode 609.000000, reward total was -20.000000. running mean: -20.178817\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.177029\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.175258\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.183506\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.191671\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.189754\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.187856\n",
            "resetting env. episode 616.000000, reward total was -18.000000. running mean: -20.165978\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.164318\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.172675\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.170948\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.169239\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.177546\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.175771\n",
            "resetting env. episode 623.000000, reward total was -19.000000. running mean: -20.164013\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -20.152373\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.150849\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.159341\n",
            "resetting env. episode 627.000000, reward total was -19.000000. running mean: -20.147747\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.156270\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -20.154707\n",
            "resetting env. episode 630.000000, reward total was -20.000000. running mean: -20.153160\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.161629\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.170012\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.178312\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.176529\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.184764\n",
            "resetting env. episode 636.000000, reward total was -19.000000. running mean: -20.172916\n",
            "resetting env. episode 637.000000, reward total was -18.000000. running mean: -20.151187\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.149675\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.158178\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.156596\n",
            "resetting env. episode 641.000000, reward total was -19.000000. running mean: -20.145031\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.143580\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.152144\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.150623\n",
            "resetting env. episode 645.000000, reward total was -20.000000. running mean: -20.149117\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.157626\n",
            "resetting env. episode 647.000000, reward total was -18.000000. running mean: -20.136049\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.144689\n",
            "resetting env. episode 649.000000, reward total was -19.000000. running mean: -20.133242\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.141910\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.150490\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.148986\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.147496\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.146021\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.154561\n",
            "resetting env. episode 656.000000, reward total was -19.000000. running mean: -20.143015\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.141585\n",
            "resetting env. episode 658.000000, reward total was -20.000000. running mean: -20.140169\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.138767\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.137380\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.136006\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.134646\n",
            "resetting env. episode 663.000000, reward total was -21.000000. running mean: -20.143299\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.151866\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.150348\n",
            "resetting env. episode 666.000000, reward total was -19.000000. running mean: -20.138844\n",
            "resetting env. episode 667.000000, reward total was -19.000000. running mean: -20.127456\n",
            "resetting env. episode 668.000000, reward total was -19.000000. running mean: -20.116181\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.115019\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.123869\n",
            "resetting env. episode 671.000000, reward total was -19.000000. running mean: -20.112630\n",
            "resetting env. episode 672.000000, reward total was -18.000000. running mean: -20.091504\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.090589\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.099683\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.108686\n",
            "resetting env. episode 676.000000, reward total was -18.000000. running mean: -20.087599\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.086723\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.095856\n",
            "resetting env. episode 679.000000, reward total was -19.000000. running mean: -20.084898\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.094049\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.103108\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.102077\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.111056\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.109946\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.118846\n",
            "resetting env. episode 686.000000, reward total was -17.000000. running mean: -20.087658\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.086781\n",
            "resetting env. episode 688.000000, reward total was -18.000000. running mean: -20.065913\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.075254\n",
            "resetting env. episode 690.000000, reward total was -21.000000. running mean: -20.084502\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.083657\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.092820\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.091892\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.100973\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.099963\n",
            "resetting env. episode 696.000000, reward total was -19.000000. running mean: -20.088964\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.098074\n",
            "resetting env. episode 698.000000, reward total was -19.000000. running mean: -20.087093\n",
            "resetting env. episode 699.000000, reward total was -20.000000. running mean: -20.086222\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.095360\n",
            "resetting env. episode 701.000000, reward total was -18.000000. running mean: -20.074407\n",
            "resetting env. episode 702.000000, reward total was -19.000000. running mean: -20.063663\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.063026\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.062396\n",
            "resetting env. episode 705.000000, reward total was -17.000000. running mean: -20.031772\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.041454\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -20.031039\n",
            "resetting env. episode 708.000000, reward total was -19.000000. running mean: -20.020729\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.030522\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.040217\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.039814\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.039416\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.039022\n",
            "resetting env. episode 714.000000, reward total was -20.000000. running mean: -20.038632\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.038246\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -20.037863\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.047484\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.057010\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.056439\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.065875\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.075216\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.074464\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.083720\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.082882\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.092054\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.091133\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.100222\n",
            "resetting env. episode 728.000000, reward total was -19.000000. running mean: -20.089219\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -20.088327\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.087444\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.086570\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.085704\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.094847\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.103898\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.112859\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.111731\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.120613\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.129407\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.128113\n",
            "resetting env. episode 740.000000, reward total was -19.000000. running mean: -20.116832\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.125664\n",
            "resetting env. episode 742.000000, reward total was -19.000000. running mean: -20.114407\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.113263\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.122130\n",
            "resetting env. episode 745.000000, reward total was -19.000000. running mean: -20.110909\n",
            "resetting env. episode 746.000000, reward total was -19.000000. running mean: -20.099800\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.108802\n",
            "resetting env. episode 748.000000, reward total was -18.000000. running mean: -20.087714\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -20.086837\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.095969\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.095009\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.104059\n",
            "resetting env. episode 753.000000, reward total was -20.000000. running mean: -20.103018\n",
            "resetting env. episode 754.000000, reward total was -19.000000. running mean: -20.091988\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.081068\n",
            "resetting env. episode 756.000000, reward total was -19.000000. running mean: -20.070257\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.069555\n",
            "resetting env. episode 758.000000, reward total was -19.000000. running mean: -20.058859\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.068271\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.077588\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -20.076812\n",
            "resetting env. episode 762.000000, reward total was -20.000000. running mean: -20.076044\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.065284\n",
            "resetting env. episode 764.000000, reward total was -18.000000. running mean: -20.044631\n",
            "resetting env. episode 765.000000, reward total was -16.000000. running mean: -20.004184\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -19.994143\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -19.984201\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -19.984359\n",
            "resetting env. episode 769.000000, reward total was -20.000000. running mean: -19.984516\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -19.994670\n",
            "resetting env. episode 771.000000, reward total was -21.000000. running mean: -20.004724\n",
            "resetting env. episode 772.000000, reward total was -19.000000. running mean: -19.994676\n",
            "resetting env. episode 773.000000, reward total was -20.000000. running mean: -19.994730\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.004782\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.004735\n",
            "resetting env. episode 776.000000, reward total was -19.000000. running mean: -19.994687\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.004740\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -19.994693\n",
            "resetting env. episode 779.000000, reward total was -19.000000. running mean: -19.984746\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -19.984899\n",
            "resetting env. episode 781.000000, reward total was -20.000000. running mean: -19.985050\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -19.995199\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.005247\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.005195\n",
            "resetting env. episode 785.000000, reward total was -21.000000. running mean: -20.015143\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.014991\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.024841\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.034593\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -20.034247\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.043905\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.043465\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.053031\n",
            "resetting env. episode 793.000000, reward total was -20.000000. running mean: -20.052501\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.051975\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.061456\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.070841\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.070133\n",
            "resetting env. episode 798.000000, reward total was -18.000000. running mean: -20.049431\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.058937\n",
            "resetting env. episode 800.000000, reward total was -19.000000. running mean: -20.048348\n",
            "resetting env. episode 801.000000, reward total was -19.000000. running mean: -20.037864\n",
            "resetting env. episode 802.000000, reward total was -18.000000. running mean: -20.017486\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.027311\n",
            "resetting env. episode 804.000000, reward total was -19.000000. running mean: -20.017038\n",
            "resetting env. episode 805.000000, reward total was -20.000000. running mean: -20.016867\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.026699\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.036432\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.046067\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.055607\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.065051\n",
            "resetting env. episode 811.000000, reward total was -20.000000. running mean: -20.064400\n",
            "resetting env. episode 812.000000, reward total was -19.000000. running mean: -20.053756\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.053219\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -20.042686\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.042259\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.051837\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.061319\n",
            "resetting env. episode 818.000000, reward total was -20.000000. running mean: -20.060705\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.060098\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.069497\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -20.058802\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.068214\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.077532\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.076757\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.075989\n",
            "resetting env. episode 826.000000, reward total was -19.000000. running mean: -20.065229\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.074577\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -20.073831\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.073093\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.082362\n",
            "resetting env. episode 831.000000, reward total was -20.000000. running mean: -20.081538\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -20.080723\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.069916\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.079217\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.088424\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.087540\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.076665\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.085898\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.095039\n",
            "resetting env. episode 840.000000, reward total was -20.000000. running mean: -20.094089\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.103148\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.112116\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.110995\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -20.109885\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.108786\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.117699\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.116522\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.125356\n",
            "resetting env. episode 849.000000, reward total was -18.000000. running mean: -20.104103\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.113062\n",
            "resetting env. episode 851.000000, reward total was -20.000000. running mean: -20.111931\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.120812\n",
            "resetting env. episode 853.000000, reward total was -19.000000. running mean: -20.109604\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.118508\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -20.117323\n",
            "resetting env. episode 856.000000, reward total was -19.000000. running mean: -20.106149\n",
            "resetting env. episode 857.000000, reward total was -19.000000. running mean: -20.095088\n",
            "resetting env. episode 858.000000, reward total was -20.000000. running mean: -20.094137\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.103196\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.102164\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -20.101142\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.100131\n",
            "resetting env. episode 863.000000, reward total was -21.000000. running mean: -20.109129\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -20.108038\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.106958\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.115888\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.124729\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.133482\n",
            "resetting env. episode 869.000000, reward total was -18.000000. running mean: -20.112147\n",
            "resetting env. episode 870.000000, reward total was -19.000000. running mean: -20.101026\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.100015\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.099015\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.108025\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.116945\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.125775\n",
            "resetting env. episode 876.000000, reward total was -19.000000. running mean: -20.114518\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.123372\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.122139\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.130917\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.139608\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.138212\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.136830\n",
            "resetting env. episode 883.000000, reward total was -20.000000. running mean: -20.135462\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.144107\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.152666\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -20.151139\n",
            "resetting env. episode 887.000000, reward total was -19.000000. running mean: -20.139628\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.138232\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -20.136849\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.135481\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.144126\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.152685\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.151158\n",
            "resetting env. episode 894.000000, reward total was -19.000000. running mean: -20.139646\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.138250\n",
            "resetting env. episode 896.000000, reward total was -20.000000. running mean: -20.136867\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.145499\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.154044\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.162503\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.170878\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.169169\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -20.157478\n",
            "resetting env. episode 903.000000, reward total was -19.000000. running mean: -20.145903\n",
            "resetting env. episode 904.000000, reward total was -19.000000. running mean: -20.134444\n",
            "resetting env. episode 905.000000, reward total was -18.000000. running mean: -20.113100\n",
            "resetting env. episode 906.000000, reward total was -19.000000. running mean: -20.101969\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -20.090949\n",
            "resetting env. episode 908.000000, reward total was -18.000000. running mean: -20.070039\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.079339\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.078546\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.087760\n",
            "resetting env. episode 912.000000, reward total was -20.000000. running mean: -20.086883\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -20.086014\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.085154\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.084302\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.093459\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -20.092524\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.101599\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.110583\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.119477\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.128283\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.127000\n",
            "resetting env. episode 923.000000, reward total was -19.000000. running mean: -20.115730\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.124572\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.123327\n",
            "resetting env. episode 926.000000, reward total was -18.000000. running mean: -20.102093\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.111073\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -20.109962\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.108862\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -20.107774\n",
            "resetting env. episode 931.000000, reward total was -19.000000. running mean: -20.096696\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.105729\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.114672\n",
            "resetting env. episode 934.000000, reward total was -20.000000. running mean: -20.113525\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.122390\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.121166\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.129954\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.138655\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.147268\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.155795\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.164237\n",
            "resetting env. episode 942.000000, reward total was -20.000000. running mean: -20.162595\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.170969\n",
            "resetting env. episode 944.000000, reward total was -18.000000. running mean: -20.149259\n",
            "resetting env. episode 945.000000, reward total was -18.000000. running mean: -20.127767\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.126489\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.125224\n",
            "resetting env. episode 948.000000, reward total was -19.000000. running mean: -20.113972\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.112832\n",
            "resetting env. episode 950.000000, reward total was -20.000000. running mean: -20.111704\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.120587\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.129381\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.138087\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.136706\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.145339\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.153886\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.152347\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -20.140824\n",
            "resetting env. episode 959.000000, reward total was -20.000000. running mean: -20.139415\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.148021\n",
            "resetting env. episode 961.000000, reward total was -19.000000. running mean: -20.136541\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -20.125176\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.123924\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.122685\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.111458\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.120343\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.129140\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.127848\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.136570\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.145204\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -20.133752\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -20.122415\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.131190\n",
            "resetting env. episode 974.000000, reward total was -19.000000. running mean: -20.119878\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -20.118680\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.117493\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.126318\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.135055\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.143704\n",
            "resetting env. episode 980.000000, reward total was -19.000000. running mean: -20.132267\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.140945\n",
            "resetting env. episode 982.000000, reward total was -19.000000. running mean: -20.129535\n",
            "resetting env. episode 983.000000, reward total was -18.000000. running mean: -20.108240\n",
            "resetting env. episode 984.000000, reward total was -19.000000. running mean: -20.097157\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.106186\n",
            "resetting env. episode 986.000000, reward total was -19.000000. running mean: -20.095124\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.094173\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.103231\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.102199\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.101177\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -20.090165\n",
            "resetting env. episode 992.000000, reward total was -18.000000. running mean: -20.069263\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.078571\n",
            "resetting env. episode 994.000000, reward total was -19.000000. running mean: -20.067785\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -20.067107\n",
            "resetting env. episode 996.000000, reward total was -18.000000. running mean: -20.046436\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.055972\n",
            "resetting env. episode 998.000000, reward total was -17.000000. running mean: -20.025412\n",
            "resetting env. episode 999.000000, reward total was -19.000000. running mean: -20.015158\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.015006\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.014856\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -20.004708\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -20.004660\n",
            "resetting env. episode 1004.000000, reward total was -20.000000. running mean: -20.004614\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.014568\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.024422\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -20.014178\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.014036\n",
            "resetting env. episode 1009.000000, reward total was -20.000000. running mean: -20.013896\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.013757\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.023619\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.023383\n",
            "resetting env. episode 1013.000000, reward total was -20.000000. running mean: -20.023149\n",
            "resetting env. episode 1014.000000, reward total was -20.000000. running mean: -20.022918\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.022688\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.032462\n",
            "resetting env. episode 1017.000000, reward total was -19.000000. running mean: -20.022137\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -20.011916\n",
            "resetting env. episode 1019.000000, reward total was -18.000000. running mean: -19.991796\n",
            "resetting env. episode 1020.000000, reward total was -16.000000. running mean: -19.951879\n",
            "resetting env. episode 1021.000000, reward total was -18.000000. running mean: -19.932360\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -19.923036\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -19.933806\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -19.944468\n",
            "resetting env. episode 1025.000000, reward total was -17.000000. running mean: -19.915023\n",
            "resetting env. episode 1026.000000, reward total was -19.000000. running mean: -19.905873\n",
            "resetting env. episode 1027.000000, reward total was -19.000000. running mean: -19.896814\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -19.907846\n",
            "resetting env. episode 1029.000000, reward total was -18.000000. running mean: -19.888767\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -19.899880\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -19.910881\n",
            "resetting env. episode 1032.000000, reward total was -17.000000. running mean: -19.881772\n",
            "resetting env. episode 1033.000000, reward total was -19.000000. running mean: -19.872954\n",
            "resetting env. episode 1034.000000, reward total was -18.000000. running mean: -19.854225\n",
            "resetting env. episode 1035.000000, reward total was -20.000000. running mean: -19.855683\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -19.867126\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -19.878455\n",
            "resetting env. episode 1038.000000, reward total was -20.000000. running mean: -19.879670\n",
            "resetting env. episode 1039.000000, reward total was -18.000000. running mean: -19.860873\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -19.862265\n",
            "resetting env. episode 1041.000000, reward total was -18.000000. running mean: -19.843642\n",
            "resetting env. episode 1042.000000, reward total was -19.000000. running mean: -19.835206\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -19.836853\n",
            "resetting env. episode 1044.000000, reward total was -19.000000. running mean: -19.828485\n",
            "resetting env. episode 1045.000000, reward total was -19.000000. running mean: -19.820200\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -19.831998\n",
            "resetting env. episode 1047.000000, reward total was -18.000000. running mean: -19.813678\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -19.825541\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -19.827286\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -19.829013\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -19.840723\n",
            "resetting env. episode 1052.000000, reward total was -19.000000. running mean: -19.832316\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -19.843993\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -19.845553\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -19.847097\n",
            "resetting env. episode 1056.000000, reward total was -18.000000. running mean: -19.828626\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -19.820340\n",
            "resetting env. episode 1058.000000, reward total was -19.000000. running mean: -19.812136\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -19.824015\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -19.835775\n",
            "resetting env. episode 1061.000000, reward total was -20.000000. running mean: -19.837417\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -19.849043\n",
            "resetting env. episode 1063.000000, reward total was -18.000000. running mean: -19.830553\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -19.822247\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -19.824025\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -19.825784\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -19.837527\n",
            "resetting env. episode 1068.000000, reward total was -18.000000. running mean: -19.819151\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -19.810960\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -19.822850\n",
            "resetting env. episode 1071.000000, reward total was -19.000000. running mean: -19.814622\n",
            "resetting env. episode 1072.000000, reward total was -19.000000. running mean: -19.806475\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -19.818411\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -19.830227\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -19.821924\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -19.833705\n",
            "resetting env. episode 1077.000000, reward total was -18.000000. running mean: -19.815368\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -19.827214\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -19.828942\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -19.830653\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -19.842346\n",
            "resetting env. episode 1082.000000, reward total was -20.000000. running mean: -19.843923\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -19.845484\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -19.857029\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -19.858458\n",
            "resetting env. episode 1086.000000, reward total was -20.000000. running mean: -19.859874\n",
            "resetting env. episode 1087.000000, reward total was -19.000000. running mean: -19.851275\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -19.852762\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -19.854235\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -19.855692\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -19.867135\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -19.858464\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -19.859879\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -19.861281\n",
            "resetting env. episode 1095.000000, reward total was -19.000000. running mean: -19.852668\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -19.854141\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -19.855600\n",
            "resetting env. episode 1098.000000, reward total was -17.000000. running mean: -19.827044\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -19.828773\n",
            "resetting env. episode 1100.000000, reward total was -16.000000. running mean: -19.790486\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -19.802581\n",
            "resetting env. episode 1102.000000, reward total was -17.000000. running mean: -19.774555\n",
            "resetting env. episode 1103.000000, reward total was -20.000000. running mean: -19.776809\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -19.789041\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -19.801151\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -19.803139\n",
            "resetting env. episode 1107.000000, reward total was -19.000000. running mean: -19.795108\n",
            "resetting env. episode 1108.000000, reward total was -19.000000. running mean: -19.787157\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -19.779285\n",
            "resetting env. episode 1110.000000, reward total was -19.000000. running mean: -19.771492\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -19.773778\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -19.786040\n",
            "resetting env. episode 1113.000000, reward total was -19.000000. running mean: -19.778179\n",
            "resetting env. episode 1114.000000, reward total was -19.000000. running mean: -19.770398\n",
            "resetting env. episode 1115.000000, reward total was -20.000000. running mean: -19.772694\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -19.774967\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -19.777217\n",
            "resetting env. episode 1118.000000, reward total was -19.000000. running mean: -19.769445\n",
            "resetting env. episode 1119.000000, reward total was -18.000000. running mean: -19.751750\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -19.754233\n",
            "resetting env. episode 1121.000000, reward total was -20.000000. running mean: -19.756691\n",
            "resetting env. episode 1122.000000, reward total was -20.000000. running mean: -19.759124\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -19.771532\n",
            "resetting env. episode 1124.000000, reward total was -20.000000. running mean: -19.773817\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -19.786079\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -19.788218\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -19.800336\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -19.812333\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -19.824209\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -19.835967\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -19.837607\n",
            "resetting env. episode 1132.000000, reward total was -15.000000. running mean: -19.789231\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -19.791339\n",
            "resetting env. episode 1134.000000, reward total was -18.000000. running mean: -19.773426\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -19.785691\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -19.787835\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -19.789956\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -19.792057\n",
            "resetting env. episode 1139.000000, reward total was -19.000000. running mean: -19.784136\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -19.786295\n",
            "resetting env. episode 1141.000000, reward total was -17.000000. running mean: -19.758432\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -19.760847\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -19.763239\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -19.765607\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -19.777951\n",
            "resetting env. episode 1146.000000, reward total was -19.000000. running mean: -19.770171\n",
            "resetting env. episode 1147.000000, reward total was -20.000000. running mean: -19.772469\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -19.774745\n",
            "resetting env. episode 1149.000000, reward total was -19.000000. running mean: -19.766997\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -19.769327\n",
            "resetting env. episode 1151.000000, reward total was -20.000000. running mean: -19.771634\n",
            "resetting env. episode 1152.000000, reward total was -19.000000. running mean: -19.763918\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -19.766278\n",
            "resetting env. episode 1154.000000, reward total was -19.000000. running mean: -19.758616\n",
            "resetting env. episode 1155.000000, reward total was -17.000000. running mean: -19.731029\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -19.733719\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -19.736382\n",
            "resetting env. episode 1158.000000, reward total was -19.000000. running mean: -19.729018\n",
            "resetting env. episode 1159.000000, reward total was -21.000000. running mean: -19.741728\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -19.754311\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -19.746768\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -19.739300\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -19.741907\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -19.754488\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -19.766943\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -19.779274\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -19.771481\n",
            "resetting env. episode 1168.000000, reward total was -21.000000. running mean: -19.783766\n",
            "resetting env. episode 1169.000000, reward total was -19.000000. running mean: -19.775928\n",
            "resetting env. episode 1170.000000, reward total was -15.000000. running mean: -19.728169\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -19.740887\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -19.743478\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -19.756044\n",
            "resetting env. episode 1174.000000, reward total was -19.000000. running mean: -19.748483\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -19.750998\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -19.753488\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -19.755954\n",
            "resetting env. episode 1178.000000, reward total was -18.000000. running mean: -19.738394\n",
            "resetting env. episode 1179.000000, reward total was -18.000000. running mean: -19.721010\n",
            "resetting env. episode 1180.000000, reward total was -18.000000. running mean: -19.703800\n",
            "resetting env. episode 1181.000000, reward total was -17.000000. running mean: -19.676762\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -19.689994\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -19.693094\n",
            "resetting env. episode 1184.000000, reward total was -18.000000. running mean: -19.676163\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -19.689402\n",
            "resetting env. episode 1186.000000, reward total was -18.000000. running mean: -19.672508\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -19.675783\n",
            "resetting env. episode 1188.000000, reward total was -18.000000. running mean: -19.659025\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -19.672435\n",
            "resetting env. episode 1190.000000, reward total was -19.000000. running mean: -19.665710\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -19.669053\n",
            "resetting env. episode 1192.000000, reward total was -18.000000. running mean: -19.652363\n",
            "resetting env. episode 1193.000000, reward total was -20.000000. running mean: -19.655839\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -19.669281\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -19.682588\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -19.685762\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -19.688904\n",
            "resetting env. episode 1198.000000, reward total was -19.000000. running mean: -19.682015\n",
            "resetting env. episode 1199.000000, reward total was -18.000000. running mean: -19.665195\n",
            "resetting env. episode 1200.000000, reward total was -19.000000. running mean: -19.658543\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -19.661958\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -19.675338\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -19.688585\n",
            "resetting env. episode 1204.000000, reward total was -18.000000. running mean: -19.671699\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -19.684982\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -19.678132\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -19.681351\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -19.694537\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -19.687592\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -19.690716\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -19.703809\n",
            "resetting env. episode 1212.000000, reward total was -19.000000. running mean: -19.696771\n",
            "resetting env. episode 1213.000000, reward total was -18.000000. running mean: -19.679803\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -19.683005\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -19.686175\n",
            "resetting env. episode 1216.000000, reward total was -21.000000. running mean: -19.699313\n",
            "resetting env. episode 1217.000000, reward total was -19.000000. running mean: -19.692320\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -19.705397\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -19.698343\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -19.711360\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -19.724246\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -19.737003\n",
            "resetting env. episode 1223.000000, reward total was -19.000000. running mean: -19.729633\n",
            "resetting env. episode 1224.000000, reward total was -19.000000. running mean: -19.722337\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -19.725114\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -19.737863\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -19.750484\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -19.752979\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -19.765449\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -19.777795\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -19.790017\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -19.802117\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -19.814096\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -19.815955\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -19.817795\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -19.829617\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -19.821321\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -19.823108\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -19.824877\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -19.836628\n",
            "resetting env. episode 1241.000000, reward total was -15.000000. running mean: -19.788262\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -19.780379\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -19.782575\n",
            "resetting env. episode 1244.000000, reward total was -19.000000. running mean: -19.774749\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -19.767002\n",
            "resetting env. episode 1246.000000, reward total was -20.000000. running mean: -19.769332\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -19.781639\n",
            "resetting env. episode 1248.000000, reward total was -18.000000. running mean: -19.763822\n",
            "resetting env. episode 1249.000000, reward total was -19.000000. running mean: -19.756184\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -19.758622\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -19.761036\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -19.773426\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -19.785691\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -19.797834\n",
            "resetting env. episode 1255.000000, reward total was -18.000000. running mean: -19.779856\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -19.782058\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -19.784237\n",
            "resetting env. episode 1258.000000, reward total was -17.000000. running mean: -19.756395\n",
            "resetting env. episode 1259.000000, reward total was -18.000000. running mean: -19.738831\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -19.741442\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -19.744028\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -19.746588\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -19.739122\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -19.741731\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -19.744313\n",
            "resetting env. episode 1266.000000, reward total was -18.000000. running mean: -19.726870\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -19.729601\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -19.732305\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -19.744982\n",
            "resetting env. episode 1270.000000, reward total was -18.000000. running mean: -19.727532\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -19.730257\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -19.732955\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -19.735625\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -19.738269\n",
            "resetting env. episode 1275.000000, reward total was -18.000000. running mean: -19.720886\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -19.723677\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -19.736440\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -19.739076\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -19.751685\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -19.754168\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -19.766627\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -19.768961\n",
            "resetting env. episode 1283.000000, reward total was -17.000000. running mean: -19.741271\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -19.743858\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -19.746420\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -19.758955\n",
            "resetting env. episode 1287.000000, reward total was -20.000000. running mean: -19.761366\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -19.763752\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -19.776115\n",
            "resetting env. episode 1290.000000, reward total was -19.000000. running mean: -19.768354\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -19.780670\n",
            "resetting env. episode 1292.000000, reward total was -18.000000. running mean: -19.762863\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -19.775235\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -19.767482\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -19.779807\n",
            "resetting env. episode 1296.000000, reward total was -19.000000. running mean: -19.772009\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -19.784289\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -19.796446\n",
            "resetting env. episode 1299.000000, reward total was -20.000000. running mean: -19.798482\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -19.800497\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -19.812492\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -19.824367\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -19.836124\n",
            "resetting env. episode 1304.000000, reward total was -18.000000. running mean: -19.817762\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -19.819585\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -19.831389\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -19.843075\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -19.854644\n",
            "resetting env. episode 1309.000000, reward total was -18.000000. running mean: -19.836098\n",
            "resetting env. episode 1310.000000, reward total was -19.000000. running mean: -19.827737\n",
            "resetting env. episode 1311.000000, reward total was -18.000000. running mean: -19.809459\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -19.821365\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -19.833151\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -19.844820\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -19.846371\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -19.857908\n",
            "resetting env. episode 1317.000000, reward total was -17.000000. running mean: -19.829329\n",
            "resetting env. episode 1318.000000, reward total was -18.000000. running mean: -19.811035\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -19.812925\n",
            "resetting env. episode 1320.000000, reward total was -19.000000. running mean: -19.804796\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -19.806748\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -19.818680\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -19.820494\n",
            "resetting env. episode 1324.000000, reward total was -18.000000. running mean: -19.802289\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -19.814266\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -19.826123\n",
            "resetting env. episode 1327.000000, reward total was -18.000000. running mean: -19.807862\n",
            "resetting env. episode 1328.000000, reward total was -20.000000. running mean: -19.809783\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -19.811685\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -19.813569\n",
            "resetting env. episode 1331.000000, reward total was -18.000000. running mean: -19.795433\n",
            "resetting env. episode 1332.000000, reward total was -19.000000. running mean: -19.787479\n",
            "resetting env. episode 1333.000000, reward total was -18.000000. running mean: -19.769604\n",
            "resetting env. episode 1334.000000, reward total was -20.000000. running mean: -19.771908\n",
            "resetting env. episode 1335.000000, reward total was -19.000000. running mean: -19.764189\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -19.766547\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -19.778881\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -19.791092\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -19.793182\n",
            "resetting env. episode 1340.000000, reward total was -20.000000. running mean: -19.795250\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -19.807297\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -19.809224\n",
            "resetting env. episode 1343.000000, reward total was -18.000000. running mean: -19.791132\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -19.793221\n",
            "resetting env. episode 1345.000000, reward total was -20.000000. running mean: -19.795288\n",
            "resetting env. episode 1346.000000, reward total was -19.000000. running mean: -19.787336\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -19.789462\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -19.801568\n",
            "resetting env. episode 1349.000000, reward total was -19.000000. running mean: -19.793552\n",
            "resetting env. episode 1350.000000, reward total was -19.000000. running mean: -19.785616\n",
            "resetting env. episode 1351.000000, reward total was -20.000000. running mean: -19.787760\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -19.779883\n",
            "resetting env. episode 1353.000000, reward total was -19.000000. running mean: -19.772084\n",
            "resetting env. episode 1354.000000, reward total was -19.000000. running mean: -19.764363\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -19.776719\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -19.768952\n",
            "resetting env. episode 1357.000000, reward total was -18.000000. running mean: -19.751263\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -19.763750\n",
            "resetting env. episode 1359.000000, reward total was -19.000000. running mean: -19.756113\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -19.768551\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -19.780866\n",
            "resetting env. episode 1362.000000, reward total was -18.000000. running mean: -19.763057\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -19.765427\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -19.767772\n",
            "resetting env. episode 1365.000000, reward total was -19.000000. running mean: -19.760095\n",
            "resetting env. episode 1366.000000, reward total was -20.000000. running mean: -19.762494\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -19.774869\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -19.787120\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -19.799249\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -19.811256\n",
            "resetting env. episode 1371.000000, reward total was -19.000000. running mean: -19.803144\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -19.815112\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -19.826961\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -19.828692\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -19.830405\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -19.832101\n",
            "resetting env. episode 1377.000000, reward total was -16.000000. running mean: -19.793780\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -19.795842\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -19.787883\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -19.790005\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -19.802105\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -19.814084\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -19.815943\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -19.827783\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -19.839505\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -19.851110\n",
            "resetting env. episode 1387.000000, reward total was -19.000000. running mean: -19.842599\n",
            "resetting env. episode 1388.000000, reward total was -18.000000. running mean: -19.824173\n",
            "resetting env. episode 1389.000000, reward total was -16.000000. running mean: -19.785932\n",
            "resetting env. episode 1390.000000, reward total was -17.000000. running mean: -19.758072\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -19.770492\n",
            "resetting env. episode 1392.000000, reward total was -20.000000. running mean: -19.772787\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -19.775059\n",
            "resetting env. episode 1394.000000, reward total was -19.000000. running mean: -19.767308\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -19.769635\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -19.781939\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -19.784119\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -19.796278\n",
            "resetting env. episode 1399.000000, reward total was -19.000000. running mean: -19.788315\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -19.800432\n",
            "resetting env. episode 1401.000000, reward total was -19.000000. running mean: -19.792428\n",
            "resetting env. episode 1402.000000, reward total was -19.000000. running mean: -19.784504\n",
            "resetting env. episode 1403.000000, reward total was -18.000000. running mean: -19.766659\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -19.778992\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -19.781202\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -19.783390\n",
            "resetting env. episode 1407.000000, reward total was -18.000000. running mean: -19.765556\n",
            "resetting env. episode 1408.000000, reward total was -17.000000. running mean: -19.737901\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -19.750522\n",
            "resetting env. episode 1410.000000, reward total was -19.000000. running mean: -19.743016\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -19.745586\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -19.748130\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -19.760649\n",
            "resetting env. episode 1414.000000, reward total was -19.000000. running mean: -19.753043\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -19.755512\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -19.767957\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -19.780277\n",
            "resetting env. episode 1418.000000, reward total was -20.000000. running mean: -19.782475\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -19.774650\n",
            "resetting env. episode 1420.000000, reward total was -18.000000. running mean: -19.756903\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -19.759334\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -19.761741\n",
            "resetting env. episode 1423.000000, reward total was -16.000000. running mean: -19.724124\n",
            "resetting env. episode 1424.000000, reward total was -19.000000. running mean: -19.716882\n",
            "resetting env. episode 1425.000000, reward total was -18.000000. running mean: -19.699714\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -19.712716\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -19.725589\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -19.738333\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -19.750950\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -19.753441\n",
            "resetting env. episode 1431.000000, reward total was -19.000000. running mean: -19.745906\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -19.758447\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -19.760863\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -19.763254\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -19.775621\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -19.787865\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -19.799987\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -19.811987\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -19.813867\n",
            "resetting env. episode 1440.000000, reward total was -19.000000. running mean: -19.805728\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -19.807671\n",
            "resetting env. episode 1442.000000, reward total was -19.000000. running mean: -19.799594\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -19.801598\n",
            "resetting env. episode 1444.000000, reward total was -18.000000. running mean: -19.783582\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -19.795746\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -19.797789\n",
            "resetting env. episode 1447.000000, reward total was -17.000000. running mean: -19.769811\n",
            "resetting env. episode 1448.000000, reward total was -18.000000. running mean: -19.752113\n",
            "resetting env. episode 1449.000000, reward total was -19.000000. running mean: -19.744592\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -19.757146\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -19.769574\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -19.781879\n",
            "resetting env. episode 1453.000000, reward total was -19.000000. running mean: -19.774060\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -19.786319\n",
            "resetting env. episode 1455.000000, reward total was -21.000000. running mean: -19.798456\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -19.800472\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -19.812467\n",
            "resetting env. episode 1458.000000, reward total was -19.000000. running mean: -19.804342\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -19.806299\n",
            "resetting env. episode 1460.000000, reward total was -19.000000. running mean: -19.798236\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -19.800253\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -19.802251\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -19.814228\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -19.826086\n",
            "resetting env. episode 1465.000000, reward total was -19.000000. running mean: -19.817825\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -19.819647\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -19.831451\n",
            "resetting env. episode 1468.000000, reward total was -18.000000. running mean: -19.813136\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -19.805005\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -19.806955\n",
            "resetting env. episode 1471.000000, reward total was -20.000000. running mean: -19.808885\n",
            "resetting env. episode 1472.000000, reward total was -20.000000. running mean: -19.810796\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -19.812688\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -19.824561\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -19.836316\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -19.847953\n",
            "resetting env. episode 1477.000000, reward total was -17.000000. running mean: -19.819473\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -19.801278\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -19.813266\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -19.815133\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -19.816982\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -19.818812\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -19.820624\n",
            "resetting env. episode 1484.000000, reward total was -19.000000. running mean: -19.812417\n",
            "resetting env. episode 1485.000000, reward total was -19.000000. running mean: -19.804293\n",
            "resetting env. episode 1486.000000, reward total was -19.000000. running mean: -19.796250\n",
            "resetting env. episode 1487.000000, reward total was -17.000000. running mean: -19.768288\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.770605\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -19.772899\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -19.785170\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -19.797318\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -19.799345\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -19.801352\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -19.803338\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -19.815305\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -19.817152\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -19.818980\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -19.830790\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.832482\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -19.834158\n",
            "CPU times: user 2h 54min 57s, sys: 35min 53s, total: 3h 30min 51s\n",
            "Wall time: 1h 47min 12s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "39a3f338-2ba1-4a45-913c-ac6f3a057655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}