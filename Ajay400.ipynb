{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ajay400.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "777d6d99-4506-478b-d196-dec230cb4377"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=c65a88f3e0bce5fcaedef16b370d12b87ec54a6279b4a5b953b3009024680583\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "c5f70024-9d5a-4984-c80e-f1d8fcd24dec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "43919ba3-a0d4-480a-a648-cddb600d67ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "42c48da3-bc43-4af4-e55c-abcc96daee13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "563c7356-23ef-4f40-8b47-34219d910f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -12.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 400 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "cc08ed90-2047-4d11-e239-6d0e6efb1ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019801\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.029603\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.039307\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.048914\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.058425\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.067841\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.077162\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.076390\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.085627\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.084770\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.093923\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.082983\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.082154\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.091332\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.100419\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.099415\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.108420\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.117336\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.126163\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.134901\n",
            "resetting env. episode 24.000000, reward total was -15.000000. running mean: -20.083552\n",
            "resetting env. episode 25.000000, reward total was -19.000000. running mean: -20.072717\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.071989\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.081270\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.090457\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.099552\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.108557\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.117471\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.106297\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.115234\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.114081\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.112940\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.121811\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.130593\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.139287\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.137894\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.146515\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.155050\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.163499\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.171864\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.170146\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.178444\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.176660\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.184893\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.183044\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.171214\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.179502\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.187707\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.195830\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.183871\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.192033\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.180112\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.188311\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.196428\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.204464\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.212419\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.220295\n",
            "resetting env. episode 61.000000, reward total was -18.000000. running mean: -20.198092\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.206111\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.204050\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.212010\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.209889\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.217791\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.225613\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.233357\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.241023\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.238613\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.246227\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.243764\n",
            "resetting env. episode 73.000000, reward total was -19.000000. running mean: -20.231327\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.239013\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.226623\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.234357\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.242014\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.249593\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.247097\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.244626\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.242180\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.239758\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.227361\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.225087\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.232836\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.240508\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.248103\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.255622\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.263066\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.260435\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.267831\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.265152\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.272501\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -20.259776\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.257178\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.264606\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.271960\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.269241\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.266548\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.273883\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.281144\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.278332\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.285549\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.292694\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.289767\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.296869\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.283900\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.281061\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.288251\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.285368\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.282515\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.289689\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.286792\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.283925\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.281085\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.288274\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.295392\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.292438\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.299513\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.296518\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.303553\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.310518\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.317412\n",
            "resetting env. episode 124.000000, reward total was -19.000000. running mean: -20.304238\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.311196\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.318084\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.324903\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.331654\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.338338\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.324954\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.331705\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.338388\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.345004\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.351554\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.348038\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.354558\n",
            "resetting env. episode 137.000000, reward total was -18.000000. running mean: -20.331012\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.327702\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.334425\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.341081\n",
            "resetting env. episode 141.000000, reward total was -18.000000. running mean: -20.317670\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.314493\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.311348\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.308235\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.315152\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.312001\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.318881\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.325692\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.332435\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.339111\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.335720\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -20.322363\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.329139\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.335848\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.342489\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.349064\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.355574\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.362018\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.368398\n",
            "resetting env. episode 160.000000, reward total was -18.000000. running mean: -20.344714\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.351267\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.357754\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.354176\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.360635\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.357028\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.363458\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.369823\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.376125\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.382364\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.368540\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.364855\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.361206\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.367594\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.373918\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.380179\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.376377\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.372614\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.378887\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.385099\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.381248\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.377435\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.383661\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.369824\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.366126\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.372465\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.368740\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.365053\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.371402\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.377688\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.373911\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.370172\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.376470\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.362706\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.359079\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.365488\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.361833\n",
            "resetting env. episode 197.000000, reward total was -16.000000. running mean: -20.318215\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.325032\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.331782\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.318464\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.315280\n",
            "resetting env. episode 202.000000, reward total was -18.000000. running mean: -20.292127\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.299206\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.286213\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.293351\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.300418\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.307414\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.304340\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.311296\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.318183\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.305001\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.311951\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.318832\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.325643\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.322387\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.329163\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.325872\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.332613\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.339287\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.345894\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.342435\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.339011\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.345620\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.342164\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.348743\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.355255\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.361703\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.368086\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.374405\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.380661\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.376854\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.383086\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.389255\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.395362\n",
            "resetting env. episode 235.000000, reward total was -18.000000. running mean: -20.371409\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.377694\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.373917\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.380178\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.386377\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.382513\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.388688\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.394801\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.400853\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.406844\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.392776\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.398848\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.394860\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.400911\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.406902\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.402833\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.398804\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.404816\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.410768\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.396661\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.402694\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.408667\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.414580\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.410435\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.416330\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.422167\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.427945\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.433666\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.439329\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.444936\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.450487\n",
            "resetting env. episode 266.000000, reward total was -18.000000. running mean: -20.425982\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.431722\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.437405\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.433031\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.438700\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.444313\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.439870\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.435471\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.421117\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.416906\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.412736\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.418609\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.424423\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.430179\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.435877\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.441518\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.447103\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.452632\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.428106\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.413825\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.419686\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.425490\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.421235\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.427022\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.422752\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.418525\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.414339\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.420196\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.425994\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.431734\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.417417\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.403243\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.409210\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.415118\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.420967\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.426757\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.432490\n",
            "resetting env. episode 303.000000, reward total was -19.000000. running mean: -20.418165\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.403983\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.409943\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.415844\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.411685\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.407568\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.403493\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.409458\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.415363\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.421210\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.426998\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.412728\n",
            "resetting env. episode 315.000000, reward total was -18.000000. running mean: -20.388600\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.394714\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.390767\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.396859\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.402891\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.388862\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.384973\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.391124\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.397212\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.403240\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.399208\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.395216\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.401264\n",
            "resetting env. episode 328.000000, reward total was -18.000000. running mean: -20.377251\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.373478\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.369744\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.376046\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.382286\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.378463\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.374678\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.380932\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.387122\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.393251\n",
            "resetting env. episode 338.000000, reward total was -18.000000. running mean: -20.369318\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.375625\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.371869\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.378150\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.384369\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.380525\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.386720\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.382853\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.389024\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.375134\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.381383\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.387569\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.383693\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.389856\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.385958\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.392098\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.398177\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.404195\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.410153\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.406052\n",
            "resetting env. episode 358.000000, reward total was -19.000000. running mean: -20.391991\n",
            "resetting env. episode 359.000000, reward total was -19.000000. running mean: -20.378071\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.384291\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.370448\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.376743\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.372976\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.369246\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.375554\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.381798\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.377980\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.384200\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.390358\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.376455\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.372690\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.378963\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.385174\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.391322\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.397409\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.393435\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.389500\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.395605\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.401649\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.407633\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.403556\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.409521\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.415426\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.421271\n",
            "resetting env. episode 385.000000, reward total was -16.000000. running mean: -20.377059\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.373288\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.359555\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.355960\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.352400\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.348876\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.345387\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.351933\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.358414\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.364830\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.371182\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.377470\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.383695\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.389858\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.385960\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.382100\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.368279\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.364596\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.360950\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.357341\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.353767\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.340230\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.346827\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.333359\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.340025\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.346625\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.353159\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.359627\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.346031\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.342571\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.339145\n",
            "resetting env. episode 416.000000, reward total was -19.000000. running mean: -20.325754\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.332496\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.339171\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.335779\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.342422\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.328997\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.325707\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.332450\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.319126\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.315935\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.322775\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.329547\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.326252\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.332989\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.329660\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.316363\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.323199\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.329967\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.336668\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.333301\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.339968\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.346568\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.353103\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.349572\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.356076\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.362515\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.368890\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.355201\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.361649\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.368033\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.374352\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.360609\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.367003\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.353333\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.349799\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.356301\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.362738\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.359111\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.365520\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.361865\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.348246\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.344764\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.351316\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.357803\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.354225\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.350682\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.357176\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.363604\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.369968\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.376268\n",
            "resetting env. episode 466.000000, reward total was -18.000000. running mean: -20.352505\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.358980\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.355391\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.361837\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.368218\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.374536\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.370791\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.367083\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.373412\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.379678\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.385881\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.392022\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.388102\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.394221\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.400279\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.406276\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.402213\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.408191\n",
            "resetting env. episode 484.000000, reward total was -18.000000. running mean: -20.384109\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.390268\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.396366\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.402402\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.398378\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.384394\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.390550\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.396645\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.392678\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.398751\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.394764\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.400816\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.386808\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.392940\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.389011\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.395120\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.391169\n",
            "CPU times: user 36min 52s, sys: 11min 44s, total: 48min 36s\n",
            "Wall time: 25min 14s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "deb6696b-0bca-4bab-c899-4fc006a88c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 4.000000, reward total was -19.000000. running mean: -20.970100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.970399\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -20.950695\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.951188\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.941676\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.922259\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.923037\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.913806\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.914668\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.895522\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.876566\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.867801\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.869123\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.870432\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.871727\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.863010\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.864380\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.855736\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.857179\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.848607\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.840121\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.841720\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.833302\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.834969\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.836620\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.838254\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.829871\n",
            "resetting env. episode 31.000000, reward total was -18.000000. running mean: -20.801572\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.793557\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.795621\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.787665\n",
            "resetting env. episode 35.000000, reward total was -18.000000. running mean: -20.759788\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.752190\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.754668\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.757122\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.739550\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.742155\n",
            "resetting env. episode 41.000000, reward total was -18.000000. running mean: -20.714733\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.717586\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.720410\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.723206\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.725974\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.728714\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.721427\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.724213\n",
            "resetting env. episode 49.000000, reward total was -17.000000. running mean: -20.686971\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.690101\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.673200\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.676468\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.679703\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.682906\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -20.656077\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.649517\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.653021\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.646491\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.640026\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.633626\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.637290\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.640917\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.634508\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.638163\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.641781\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.635363\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.629009\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.632719\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.626392\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.620128\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.613927\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.617788\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.611610\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.595494\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.599539\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.593543\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.587608\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.591732\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.595815\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.599856\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.603858\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.607819\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.591741\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.595824\n",
            "resetting env. episode 85.000000, reward total was -18.000000. running mean: -20.569865\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.564167\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.568525\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.552840\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.557311\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.561738\n",
            "resetting env. episode 91.000000, reward total was -18.000000. running mean: -20.536121\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.540760\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.525352\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.530099\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.534798\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.529450\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.534155\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.538814\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.523426\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.528191\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.532909\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.537580\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.532204\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.536882\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.541514\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.546098\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.550637\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.545131\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.539680\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.534283\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.538940\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.533551\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.528215\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.532933\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.527604\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.522328\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -20.507104\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.502033\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.507013\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.511943\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.506824\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.511755\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.516638\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.521471\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.516257\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.521094\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.505883\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.490824\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.495916\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.500957\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.495947\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.500988\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.505978\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.500918\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.505909\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.500850\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.505841\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -20.490783\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.495875\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.500916\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.505907\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.490848\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.495940\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.480980\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.486170\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.481309\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.486496\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.481631\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.486814\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -20.471946\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.457227\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.462655\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.468028\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.473348\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.468614\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.473928\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.479189\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.484397\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.479553\n",
            "resetting env. episode 160.000000, reward total was -19.000000. running mean: -20.464757\n",
            "resetting env. episode 161.000000, reward total was -20.000000. running mean: -20.460110\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.465509\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.470854\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.476145\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.481384\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.486570\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.491704\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.476787\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.482019\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.487199\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.492327\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.497404\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.482430\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.487605\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.482729\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.477902\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.483123\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.488292\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.483409\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.488575\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.493689\n",
            "resetting env. episode 182.000000, reward total was -18.000000. running mean: -20.468752\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.474065\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.479324\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.484531\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.489685\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.494789\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.489841\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.484942\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.480093\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.465292\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.460639\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.456033\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.461472\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.466858\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.462189\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.467567\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.462891\n",
            "resetting env. episode 199.000000, reward total was -17.000000. running mean: -20.428263\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.433980\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.439640\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.425244\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.430991\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.426681\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.432415\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.438090\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.423710\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.429472\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.435178\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.430826\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.436518\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.442153\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.447731\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.453254\n",
            "resetting env. episode 215.000000, reward total was -19.000000. running mean: -20.438721\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.444334\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.439891\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.435492\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.441137\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.436725\n",
            "resetting env. episode 221.000000, reward total was -18.000000. running mean: -20.412358\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.408235\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.394152\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.400211\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.406209\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.402146\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.408125\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.404044\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.410003\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.415903\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.421744\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.427527\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.433252\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.428919\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.434630\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.440284\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.425881\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.421622\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.417406\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.423232\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.418999\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.414809\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.420661\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.426455\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.432190\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.437868\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.433489\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.429155\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.414863\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.400714\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.406707\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.412640\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.418514\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.414329\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.420185\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.425984\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.421724\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.417506\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.423331\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.419098\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.414907\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.420758\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.426550\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.412285\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.418162\n",
            "resetting env. episode 266.000000, reward total was -17.000000. running mean: -20.383980\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.380141\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.376339\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.372576\n",
            "resetting env. episode 270.000000, reward total was -17.000000. running mean: -20.338850\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.345462\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.332007\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.338687\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.335300\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.341947\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.338528\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.345142\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.331691\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.338374\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.344990\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.341540\n",
            "resetting env. episode 282.000000, reward total was -18.000000. running mean: -20.318125\n",
            "resetting env. episode 283.000000, reward total was -18.000000. running mean: -20.294944\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.291994\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.279074\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.286284\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.293421\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.290487\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.297582\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.304606\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.311560\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.318444\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.325260\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.322007\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.318787\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.305599\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.302543\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.309518\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.316423\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.323258\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.330026\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.326726\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.333458\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.340124\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.336722\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.343355\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.349922\n",
            "resetting env. episode 308.000000, reward total was -19.000000. running mean: -20.336422\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.343058\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.349628\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.356131\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.352570\n",
            "resetting env. episode 313.000000, reward total was -16.000000. running mean: -20.309044\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.305954\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.312894\n",
            "resetting env. episode 316.000000, reward total was -18.000000. running mean: -20.289765\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.276868\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.274099\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.271358\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.268645\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.275958\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.283199\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.270367\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.267663\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.274986\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.272236\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.279514\n",
            "resetting env. episode 328.000000, reward total was -18.000000. running mean: -20.256719\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.244152\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.251710\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.259193\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.266601\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.273935\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.271196\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.268484\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.265799\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.273141\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.260410\n",
            "resetting env. episode 339.000000, reward total was -18.000000. running mean: -20.237805\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.245427\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.242973\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.240543\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.248138\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.255657\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.243100\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.250669\n",
            "resetting env. episode 347.000000, reward total was -18.000000. running mean: -20.228162\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.225881\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.233622\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.241286\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.248873\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.246384\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.233920\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.241581\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.249165\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.256674\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.244107\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.241666\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.249249\n",
            "resetting env. episode 360.000000, reward total was -18.000000. running mean: -20.226757\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.234489\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.242144\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.249723\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.257226\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.264653\n",
            "resetting env. episode 366.000000, reward total was -17.000000. running mean: -20.232007\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.239687\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.247290\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.234817\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.242469\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.250044\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.257544\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.264968\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.272318\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.269595\n",
            "resetting env. episode 376.000000, reward total was -18.000000. running mean: -20.246899\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.254430\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.261886\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.249267\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.256775\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.254207\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.251665\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.259148\n",
            "resetting env. episode 384.000000, reward total was -18.000000. running mean: -20.236557\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.244191\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.251749\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.259232\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.256639\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.264073\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.271432\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.268718\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.276031\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.263270\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.270638\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.277931\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.285152\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.272300\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.279577\n",
            "resetting env. episode 399.000000, reward total was -18.000000. running mean: -20.256782\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.244214\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.241772\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.239354\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.246960\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.254491\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.261946\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.259326\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.266733\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.264066\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.271425\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.268711\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.266024\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.253364\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.240830\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.248422\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.255937\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.263378\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.270744\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.278037\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.265257\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.262604\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.269978\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.257278\n",
            "resetting env. episode 423.000000, reward total was -18.000000. running mean: -20.234705\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.242358\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.249935\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.257435\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.264861\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -20.242212\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.239790\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.237392\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.235018\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.232668\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.240342\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.247938\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.245459\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.243004\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.250574\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.248068\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.245588\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.253132\n",
            "resetting env. episode 441.000000, reward total was -19.000000. running mean: -20.240601\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.248195\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.235713\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.233355\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.241022\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.248612\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.236126\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.243764\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.241327\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.248913\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.236424\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.234060\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.231719\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.219402\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.227208\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.224936\n",
            "resetting env. episode 457.000000, reward total was -17.000000. running mean: -20.192687\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.190760\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.198852\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.206864\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.214795\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.212647\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.220521\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.228316\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.236032\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.243672\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.241235\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.248823\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.256335\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.253771\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.241234\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.248821\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.256333\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.263770\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.251132\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.258621\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.266035\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.263374\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.270740\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.268033\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.275353\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.272599\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.259873\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.267274\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.274602\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.271856\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.279137\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.286346\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.273482\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.270748\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.278040\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.265260\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.272607\n",
            "resetting env. episode 494.000000, reward total was -19.000000. running mean: -20.259881\n",
            "resetting env. episode 495.000000, reward total was -19.000000. running mean: -20.247282\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.244809\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.252361\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.259838\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.257239\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.254667\n",
            "CPU times: user 38min 11s, sys: 12min 4s, total: 50min 16s\n",
            "Wall time: 25min 56s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "5fb76b9e-0354-4d72-a9bf-ea7d5168a826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "2d00d295-44f6-4834-f6df-b375ae9b0875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.019801\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.029603\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.029307\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.029014\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.038724\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.048337\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.057853\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.057275\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.046702\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.046235\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.045773\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.055315\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.044762\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.044314\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.043871\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.053432\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.062898\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.072269\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -20.071546\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.070831\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.080122\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.089321\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.088428\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.087544\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.096668\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.105702\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.114645\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.103498\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.112463\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.121338\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.130125\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.128824\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.137536\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.136160\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.144799\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.153351\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.151817\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.160299\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.158696\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.157109\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.165538\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.163883\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.172244\n",
            "resetting env. episode 49.000000, reward total was -18.000000. running mean: -20.150521\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.159016\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.167426\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.175752\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.163994\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.172354\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.180631\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.188824\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.196936\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.204967\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.212917\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.220788\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.228580\n",
            "resetting env. episode 62.000000, reward total was -18.000000. running mean: -20.206294\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.204231\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.202189\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.210167\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.208065\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.205985\n",
            "resetting env. episode 68.000000, reward total was -19.000000. running mean: -20.193925\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.191986\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.190066\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.188165\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -20.176284\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.174521\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.172775\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.181048\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.189237\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.187345\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.195471\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.183517\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.191682\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.179765\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.177967\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.166187\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.174526\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.162780\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -20.151152\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.149641\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.158145\n",
            "resetting env. episode 89.000000, reward total was -19.000000. running mean: -20.146563\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.135097\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.133746\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.142409\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.150985\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.159475\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.167880\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.166202\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.164540\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.162894\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.171265\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.169553\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.177857\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.166078\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.174418\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -20.162673\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.161047\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.169436\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.177742\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.175964\n",
            "resetting env. episode 109.000000, reward total was -18.000000. running mean: -20.154205\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.162663\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.171036\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.179326\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.177533\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.185757\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.183900\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.192061\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.200140\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.198139\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.206157\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.204096\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.212055\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.219934\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.227735\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.235457\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.243103\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.250672\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.248165\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.255684\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.253127\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.260595\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.267989\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.275310\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.282556\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -20.279731\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.276934\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.274164\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.271423\n",
            "resetting env. episode 138.000000, reward total was -19.000000. running mean: -20.258708\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.256121\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.243560\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.251124\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.248613\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.246127\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.253666\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.241129\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.238718\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.236331\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.233967\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.241628\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.239211\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.236819\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.244451\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -20.242007\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.249587\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.257091\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.264520\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.261875\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.259256\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -20.246663\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.244197\n",
            "resetting env. episode 161.000000, reward total was -19.000000. running mean: -20.231755\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.239437\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.237043\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.244672\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.242226\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.249803\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.247305\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.254832\n",
            "resetting env. episode 169.000000, reward total was -17.000000. running mean: -20.222284\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.220061\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.227861\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.235582\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.233226\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.240894\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.248485\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.256000\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.263440\n",
            "resetting env. episode 178.000000, reward total was -18.000000. running mean: -20.240806\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.248398\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.255914\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.253354\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.250821\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.248313\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.255830\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.263271\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.260639\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.268032\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.275352\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.272598\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.269872\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.277174\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.274402\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.271658\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.258941\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.266352\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.273688\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.270951\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.278242\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.275460\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.272705\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.269978\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.267278\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.254605\n",
            "resetting env. episode 204.000000, reward total was -19.000000. running mean: -20.242059\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.249639\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.257142\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.244571\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.232125\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.239804\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.227406\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.225132\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.222881\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.200652\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.208645\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.206559\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.214493\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.222348\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.210125\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.208023\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.215943\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.223784\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.221546\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.229331\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.217037\n",
            "resetting env. episode 225.000000, reward total was -16.000000. running mean: -20.174867\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.183118\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.191287\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.199374\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.207380\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.215307\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.213154\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.211022\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.208912\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.206823\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.204754\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.192707\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.200780\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.208772\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.216684\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.204517\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.202472\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.190448\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.188543\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.196658\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.194691\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.202744\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.200717\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.198710\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.186722\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.194855\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.192907\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.200978\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.208968\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.216878\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.224709\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.232462\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.240138\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.237736\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.245359\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.252905\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.260376\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.267773\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.275095\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.272344\n",
            "resetting env. episode 265.000000, reward total was -17.000000. running mean: -20.239620\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.237224\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.234852\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.242503\n",
            "resetting env. episode 269.000000, reward total was -19.000000. running mean: -20.230078\n",
            "resetting env. episode 270.000000, reward total was -18.000000. running mean: -20.207778\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.215700\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.213543\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.201407\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.199393\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.197399\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.185425\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.193571\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.181635\n",
            "resetting env. episode 279.000000, reward total was -17.000000. running mean: -20.149819\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.138321\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.146938\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.155468\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.163914\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.172274\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.170552\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.178846\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.177058\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.185287\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.173434\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.161700\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.170083\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.168382\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.176698\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.184931\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.183082\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.181251\n",
            "resetting env. episode 297.000000, reward total was -18.000000. running mean: -20.159439\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.167844\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.176166\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.174404\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.172660\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.180934\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.179124\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.187333\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.195460\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.183505\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.191670\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.189753\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.197856\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.185877\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.174018\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.182278\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.180455\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.178651\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.186864\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.194996\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.203046\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.211015\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.218905\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.226716\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.234449\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.232104\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.229783\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.227486\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.235211\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.232859\n",
            "resetting env. episode 327.000000, reward total was -18.000000. running mean: -20.210530\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.208425\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.216341\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.224177\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.221935\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.219716\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.217519\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.225344\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.233090\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.240759\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.248352\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.255868\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.253310\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.260776\n",
            "resetting env. episode 341.000000, reward total was -18.000000. running mean: -20.238169\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.235787\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.243429\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.240995\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.248585\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.246099\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.233638\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.241302\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.248889\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.246400\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.233936\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.241596\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.249180\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.256689\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.244122\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.251681\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.259164\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.266572\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.273906\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.281167\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.288356\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.295472\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.302517\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.309492\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.316397\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.313233\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.320101\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.326900\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.313631\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.300495\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.297490\n",
            "resetting env. episode 372.000000, reward total was -18.000000. running mean: -20.274515\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.281770\n",
            "resetting env. episode 374.000000, reward total was -17.000000. running mean: -20.248952\n",
            "resetting env. episode 375.000000, reward total was -19.000000. running mean: -20.236462\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.244098\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.241657\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.249240\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.246748\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.244280\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.241838\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.249419\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.236925\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.244556\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.252110\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.249589\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.247093\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.254622\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.262076\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.269455\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.266761\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.264093\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.261452\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.248838\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.256349\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.263786\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.271148\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.278436\n",
            "resetting env. episode 399.000000, reward total was -19.000000. running mean: -20.265652\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.262996\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.270366\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.257662\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.245085\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.252634\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.260108\n",
            "resetting env. episode 406.000000, reward total was -19.000000. running mean: -20.247507\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.255032\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.242482\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.250057\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.257556\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.264981\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.272331\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.269608\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.266912\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.264242\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.271600\n",
            "resetting env. episode 417.000000, reward total was -18.000000. running mean: -20.248884\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.256395\n",
            "resetting env. episode 419.000000, reward total was -19.000000. running mean: -20.243831\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.241393\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.228979\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.226689\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.234422\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.242078\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.239657\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.247261\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.244788\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.252340\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.249817\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.237319\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.234945\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.232596\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.240270\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.237867\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.245489\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.243034\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.230603\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.238297\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.235914\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.243555\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.241120\n",
            "resetting env. episode 442.000000, reward total was -17.000000. running mean: -20.208709\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.216621\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.214455\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.222311\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.230088\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.237787\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.235409\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.223055\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.230824\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.238516\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.246131\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.243669\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.241233\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.248820\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.236332\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.233969\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.231629\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.239313\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.226920\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.224651\n",
            "resetting env. episode 462.000000, reward total was -19.000000. running mean: -20.212404\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.220280\n",
            "resetting env. episode 464.000000, reward total was -19.000000. running mean: -20.208077\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.205997\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.213937\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.201797\n",
            "resetting env. episode 468.000000, reward total was -18.000000. running mean: -20.179779\n",
            "resetting env. episode 469.000000, reward total was -18.000000. running mean: -20.157981\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.156402\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.164838\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.163189\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.161557\n",
            "resetting env. episode 474.000000, reward total was -18.000000. running mean: -20.139942\n",
            "resetting env. episode 475.000000, reward total was -18.000000. running mean: -20.118542\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.127357\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.136083\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.134723\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.143375\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.141942\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.140522\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.139117\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.137726\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.126348\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.135085\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -20.113734\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.122597\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.111371\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.110257\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.119155\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.127963\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.136683\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.135317\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.143963\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.152524\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.160998\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.159389\n",
            "resetting env. episode 498.000000, reward total was -18.000000. running mean: -20.137795\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.126417\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.115153\n",
            "resetting env. episode 501.000000, reward total was -18.000000. running mean: -20.094001\n",
            "resetting env. episode 502.000000, reward total was -20.000000. running mean: -20.093061\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.102130\n",
            "resetting env. episode 504.000000, reward total was -19.000000. running mean: -20.091109\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.100198\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.099196\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.108204\n",
            "resetting env. episode 508.000000, reward total was -21.000000. running mean: -20.117122\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.125951\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.134691\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.143344\n",
            "resetting env. episode 512.000000, reward total was -19.000000. running mean: -20.131911\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.140592\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.139186\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.147794\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -20.136316\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.144953\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.143503\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.142068\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.150648\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.159141\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.157550\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.155974\n",
            "resetting env. episode 524.000000, reward total was -19.000000. running mean: -20.144415\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.142970\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.141541\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.150125\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.158624\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.167038\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.165367\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.173714\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.181977\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.180157\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.188355\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.196472\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.204507\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.212462\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.220337\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.218134\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.225953\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.223693\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.221456\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.229242\n",
            "resetting env. episode 544.000000, reward total was -21.000000. running mean: -20.236949\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.244580\n",
            "resetting env. episode 546.000000, reward total was -20.000000. running mean: -20.242134\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.249713\n",
            "resetting env. episode 548.000000, reward total was -18.000000. running mean: -20.227215\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.234943\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.242594\n",
            "resetting env. episode 551.000000, reward total was -18.000000. running mean: -20.220168\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.217966\n",
            "resetting env. episode 553.000000, reward total was -19.000000. running mean: -20.205787\n",
            "resetting env. episode 554.000000, reward total was -18.000000. running mean: -20.183729\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.191891\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.189972\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.198073\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.196092\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.204131\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.212090\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.209969\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.217869\n",
            "resetting env. episode 563.000000, reward total was -20.000000. running mean: -20.215691\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.223534\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.221298\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.229085\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.236794\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.244426\n",
            "resetting env. episode 569.000000, reward total was -19.000000. running mean: -20.231982\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.239662\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.227266\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.224993\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.222743\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.230516\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.238211\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.235829\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.243470\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.251036\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.258525\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.255940\n",
            "resetting env. episode 581.000000, reward total was -20.000000. running mean: -20.253381\n",
            "resetting env. episode 582.000000, reward total was -19.000000. running mean: -20.240847\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.238438\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.236054\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.243693\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.251256\n",
            "resetting env. episode 587.000000, reward total was -19.000000. running mean: -20.238744\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.246356\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.253893\n",
            "resetting env. episode 590.000000, reward total was -19.000000. running mean: -20.241354\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.248940\n",
            "resetting env. episode 592.000000, reward total was -19.000000. running mean: -20.236451\n",
            "resetting env. episode 593.000000, reward total was -19.000000. running mean: -20.224086\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.221846\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.229627\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.227331\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.235058\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.232707\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.240380\n",
            "resetting env. episode 600.000000, reward total was -20.000000. running mean: -20.237976\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.235596\n",
            "resetting env. episode 602.000000, reward total was -19.000000. running mean: -20.223240\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.231008\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.238698\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.246311\n",
            "resetting env. episode 606.000000, reward total was -19.000000. running mean: -20.233848\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.241509\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.239094\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.246703\n",
            "resetting env. episode 610.000000, reward total was -18.000000. running mean: -20.224236\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.231994\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.239674\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.247277\n",
            "resetting env. episode 614.000000, reward total was -20.000000. running mean: -20.244804\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.242356\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.229933\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.237634\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.245257\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.242805\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.250377\n",
            "resetting env. episode 621.000000, reward total was -18.000000. running mean: -20.227873\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.235594\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.243238\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.250806\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.258298\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.265715\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.273058\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.280327\n",
            "resetting env. episode 629.000000, reward total was -19.000000. running mean: -20.267524\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.274848\n",
            "resetting env. episode 631.000000, reward total was -20.000000. running mean: -20.272100\n",
            "resetting env. episode 632.000000, reward total was -19.000000. running mean: -20.259379\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.256785\n",
            "resetting env. episode 634.000000, reward total was -19.000000. running mean: -20.244217\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.241775\n",
            "resetting env. episode 636.000000, reward total was -20.000000. running mean: -20.239357\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.246964\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.244494\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.252049\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.259529\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.266933\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.274264\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.271522\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.268806\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.276118\n",
            "resetting env. episode 646.000000, reward total was -18.000000. running mean: -20.253357\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.260823\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.268215\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.275533\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.282778\n",
            "resetting env. episode 651.000000, reward total was -20.000000. running mean: -20.279950\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.287150\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.284279\n",
            "resetting env. episode 654.000000, reward total was -20.000000. running mean: -20.281436\n",
            "resetting env. episode 655.000000, reward total was -19.000000. running mean: -20.268622\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.275936\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.273176\n",
            "resetting env. episode 658.000000, reward total was -18.000000. running mean: -20.250444\n",
            "resetting env. episode 659.000000, reward total was -21.000000. running mean: -20.257940\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.265361\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.272707\n",
            "resetting env. episode 662.000000, reward total was -21.000000. running mean: -20.279980\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -20.267180\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.274508\n",
            "resetting env. episode 665.000000, reward total was -19.000000. running mean: -20.261763\n",
            "resetting env. episode 666.000000, reward total was -20.000000. running mean: -20.259146\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.256554\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.263989\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.261349\n",
            "resetting env. episode 670.000000, reward total was -16.000000. running mean: -20.218735\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.226548\n",
            "resetting env. episode 672.000000, reward total was -19.000000. running mean: -20.214282\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.212140\n",
            "resetting env. episode 674.000000, reward total was -20.000000. running mean: -20.210018\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.217918\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.225739\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.223481\n",
            "resetting env. episode 678.000000, reward total was -19.000000. running mean: -20.211247\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.219134\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.216943\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.224773\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.222526\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.220300\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.218097\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.225916\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.223657\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.221421\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.229207\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.236914\n",
            "resetting env. episode 690.000000, reward total was -19.000000. running mean: -20.224545\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -20.212300\n",
            "resetting env. episode 692.000000, reward total was -17.000000. running mean: -20.180177\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.188375\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.196491\n",
            "resetting env. episode 695.000000, reward total was -19.000000. running mean: -20.184526\n",
            "resetting env. episode 696.000000, reward total was -18.000000. running mean: -20.162681\n",
            "resetting env. episode 697.000000, reward total was -19.000000. running mean: -20.151054\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.159544\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.167948\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.166269\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -20.164606\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.162960\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.171331\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.169617\n",
            "resetting env. episode 705.000000, reward total was -21.000000. running mean: -20.177921\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.186142\n",
            "resetting env. episode 707.000000, reward total was -20.000000. running mean: -20.184280\n",
            "resetting env. episode 708.000000, reward total was -20.000000. running mean: -20.182438\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.190613\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.188707\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.196820\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.204852\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.212803\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.220675\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.228469\n",
            "resetting env. episode 716.000000, reward total was -19.000000. running mean: -20.216184\n",
            "resetting env. episode 717.000000, reward total was -18.000000. running mean: -20.194022\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.192082\n",
            "resetting env. episode 719.000000, reward total was -21.000000. running mean: -20.200161\n",
            "resetting env. episode 720.000000, reward total was -18.000000. running mean: -20.178159\n",
            "resetting env. episode 721.000000, reward total was -20.000000. running mean: -20.176378\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.174614\n",
            "resetting env. episode 723.000000, reward total was -20.000000. running mean: -20.172868\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.171139\n",
            "resetting env. episode 725.000000, reward total was -20.000000. running mean: -20.169428\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.167733\n",
            "resetting env. episode 727.000000, reward total was -19.000000. running mean: -20.156056\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.154496\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.162951\n",
            "resetting env. episode 730.000000, reward total was -20.000000. running mean: -20.161321\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.159708\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.158111\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.166530\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.164864\n",
            "resetting env. episode 735.000000, reward total was -18.000000. running mean: -20.143216\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.141784\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.150366\n",
            "resetting env. episode 738.000000, reward total was -20.000000. running mean: -20.148862\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.147374\n",
            "resetting env. episode 740.000000, reward total was -19.000000. running mean: -20.135900\n",
            "resetting env. episode 741.000000, reward total was -19.000000. running mean: -20.124541\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.133295\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.131962\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.130643\n",
            "resetting env. episode 745.000000, reward total was -20.000000. running mean: -20.129336\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.128043\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.136763\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.145395\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.153941\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.152402\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.150878\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.159369\n",
            "resetting env. episode 753.000000, reward total was -18.000000. running mean: -20.137775\n",
            "resetting env. episode 754.000000, reward total was -20.000000. running mean: -20.136397\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.135033\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.133683\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.142346\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.150923\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.159414\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.167819\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.176141\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.184380\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.192536\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.190611\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.188705\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.196817\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -20.184849\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.193001\n",
            "resetting env. episode 769.000000, reward total was -18.000000. running mean: -20.171071\n",
            "resetting env. episode 770.000000, reward total was -19.000000. running mean: -20.159360\n",
            "resetting env. episode 771.000000, reward total was -18.000000. running mean: -20.137767\n",
            "resetting env. episode 772.000000, reward total was -18.000000. running mean: -20.116389\n",
            "resetting env. episode 773.000000, reward total was -19.000000. running mean: -20.105225\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.114173\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.123031\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.121801\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.130583\n",
            "resetting env. episode 778.000000, reward total was -21.000000. running mean: -20.139277\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.137884\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.136505\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.145140\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.153689\n",
            "resetting env. episode 783.000000, reward total was -19.000000. running mean: -20.142152\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.140730\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -20.139323\n",
            "resetting env. episode 786.000000, reward total was -20.000000. running mean: -20.137930\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.146551\n",
            "resetting env. episode 788.000000, reward total was -20.000000. running mean: -20.145085\n",
            "resetting env. episode 789.000000, reward total was -19.000000. running mean: -20.133634\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.142298\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.140875\n",
            "resetting env. episode 792.000000, reward total was -20.000000. running mean: -20.139466\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.148071\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.156591\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.165025\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.173375\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.171641\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.169924\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.178225\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.186443\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.184578\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.192733\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -20.190805\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.198897\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -20.186908\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.195039\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.203089\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.211058\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.208947\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.206858\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -20.194789\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.202841\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.210813\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.218705\n",
            "resetting env. episode 815.000000, reward total was -20.000000. running mean: -20.216518\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.224353\n",
            "resetting env. episode 817.000000, reward total was -20.000000. running mean: -20.222109\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.229888\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.227589\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.235313\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.242960\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -20.240531\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.248125\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.245644\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.243188\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.250756\n",
            "resetting env. episode 827.000000, reward total was -19.000000. running mean: -20.238248\n",
            "resetting env. episode 828.000000, reward total was -19.000000. running mean: -20.225866\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.233607\n",
            "resetting env. episode 830.000000, reward total was -20.000000. running mean: -20.231271\n",
            "resetting env. episode 831.000000, reward total was -19.000000. running mean: -20.218958\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.226769\n",
            "resetting env. episode 833.000000, reward total was -19.000000. running mean: -20.214501\n",
            "resetting env. episode 834.000000, reward total was -19.000000. running mean: -20.202356\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.210332\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.218229\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.206047\n",
            "resetting env. episode 838.000000, reward total was -19.000000. running mean: -20.193986\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.192046\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.200126\n",
            "resetting env. episode 841.000000, reward total was -20.000000. running mean: -20.198125\n",
            "resetting env. episode 842.000000, reward total was -19.000000. running mean: -20.186143\n",
            "resetting env. episode 843.000000, reward total was -19.000000. running mean: -20.174282\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.182539\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.180714\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.188907\n",
            "resetting env. episode 847.000000, reward total was -20.000000. running mean: -20.187018\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.185147\n",
            "resetting env. episode 849.000000, reward total was -19.000000. running mean: -20.173296\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.171563\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.179847\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.188049\n",
            "resetting env. episode 853.000000, reward total was -15.000000. running mean: -20.136168\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.144807\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.153359\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.161825\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.160207\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.168605\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.166919\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.165250\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.173597\n",
            "resetting env. episode 862.000000, reward total was -21.000000. running mean: -20.181861\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.180042\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.188242\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.186360\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.194496\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.202551\n",
            "resetting env. episode 868.000000, reward total was -19.000000. running mean: -20.190526\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.198620\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.206634\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.214568\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.212422\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.220298\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -20.218095\n",
            "resetting env. episode 875.000000, reward total was -19.000000. running mean: -20.205914\n",
            "resetting env. episode 876.000000, reward total was -19.000000. running mean: -20.193855\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.201916\n",
            "resetting env. episode 878.000000, reward total was -20.000000. running mean: -20.199897\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.207898\n",
            "resetting env. episode 880.000000, reward total was -17.000000. running mean: -20.175819\n",
            "resetting env. episode 881.000000, reward total was -20.000000. running mean: -20.174061\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.172320\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.180597\n",
            "resetting env. episode 884.000000, reward total was -20.000000. running mean: -20.178791\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.187003\n",
            "resetting env. episode 886.000000, reward total was -18.000000. running mean: -20.165133\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.173482\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.181747\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.189930\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.188030\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.186150\n",
            "resetting env. episode 892.000000, reward total was -19.000000. running mean: -20.174288\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.172546\n",
            "resetting env. episode 894.000000, reward total was -20.000000. running mean: -20.170820\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -20.159112\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.167521\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.175846\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.184087\n",
            "resetting env. episode 899.000000, reward total was -20.000000. running mean: -20.182246\n",
            "resetting env. episode 900.000000, reward total was -20.000000. running mean: -20.180424\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.188620\n",
            "resetting env. episode 902.000000, reward total was -18.000000. running mean: -20.166733\n",
            "resetting env. episode 903.000000, reward total was -20.000000. running mean: -20.165066\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.163415\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.171781\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.180063\n",
            "resetting env. episode 907.000000, reward total was -19.000000. running mean: -20.168263\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -20.166580\n",
            "resetting env. episode 909.000000, reward total was -19.000000. running mean: -20.154914\n",
            "resetting env. episode 910.000000, reward total was -19.000000. running mean: -20.143365\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.141932\n",
            "resetting env. episode 912.000000, reward total was -19.000000. running mean: -20.130512\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -20.129207\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.137915\n",
            "resetting env. episode 915.000000, reward total was -20.000000. running mean: -20.136536\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.145171\n",
            "resetting env. episode 917.000000, reward total was -20.000000. running mean: -20.143719\n",
            "resetting env. episode 918.000000, reward total was -20.000000. running mean: -20.142282\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.150859\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.159350\n",
            "resetting env. episode 921.000000, reward total was -19.000000. running mean: -20.147757\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.156279\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.164716\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.173069\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.181339\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -20.179525\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.187730\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.195853\n",
            "resetting env. episode 929.000000, reward total was -18.000000. running mean: -20.173894\n",
            "resetting env. episode 930.000000, reward total was -19.000000. running mean: -20.162155\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.160534\n",
            "resetting env. episode 932.000000, reward total was -19.000000. running mean: -20.148928\n",
            "resetting env. episode 933.000000, reward total was -20.000000. running mean: -20.147439\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.155965\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.164405\n",
            "resetting env. episode 936.000000, reward total was -20.000000. running mean: -20.162761\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.171133\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.179422\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.187628\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.185751\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.193894\n",
            "resetting env. episode 942.000000, reward total was -18.000000. running mean: -20.171955\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.180235\n",
            "resetting env. episode 944.000000, reward total was -21.000000. running mean: -20.188433\n",
            "resetting env. episode 945.000000, reward total was -19.000000. running mean: -20.176549\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.174783\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -20.163035\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.171405\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.169691\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.177994\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.186214\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.194352\n",
            "resetting env. episode 953.000000, reward total was -20.000000. running mean: -20.192409\n",
            "resetting env. episode 954.000000, reward total was -19.000000. running mean: -20.180484\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.188680\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.196793\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.194825\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.202877\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.210848\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.208739\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.216652\n",
            "resetting env. episode 962.000000, reward total was -18.000000. running mean: -20.194485\n",
            "resetting env. episode 963.000000, reward total was -19.000000. running mean: -20.182541\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.180715\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.188908\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.187019\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.195149\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.203197\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -20.201165\n",
            "resetting env. episode 970.000000, reward total was -19.000000. running mean: -20.189154\n",
            "resetting env. episode 971.000000, reward total was -19.000000. running mean: -20.177262\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -20.175489\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.183735\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.191897\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -20.179978\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.178178\n",
            "resetting env. episode 977.000000, reward total was -20.000000. running mean: -20.176397\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -20.174633\n",
            "resetting env. episode 979.000000, reward total was -20.000000. running mean: -20.172886\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.181158\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.189346\n",
            "resetting env. episode 982.000000, reward total was -20.000000. running mean: -20.187453\n",
            "resetting env. episode 983.000000, reward total was -18.000000. running mean: -20.165578\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.173922\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -20.172183\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.170461\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.168757\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.177069\n",
            "resetting env. episode 989.000000, reward total was -20.000000. running mean: -20.175298\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.183545\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.191710\n",
            "resetting env. episode 992.000000, reward total was -20.000000. running mean: -20.189793\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.187895\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.196016\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.204056\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.202015\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -20.189995\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.188095\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.196214\n",
            "resetting env. episode 1000.000000, reward total was -19.000000. running mean: -20.184252\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.182409\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -20.170585\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -20.168880\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.177191\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.185419\n",
            "resetting env. episode 1006.000000, reward total was -18.000000. running mean: -20.163565\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.171929\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.180210\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.188408\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.186524\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.194658\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.192712\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.200785\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.208777\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.216689\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.214522\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.212377\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.220253\n",
            "resetting env. episode 1019.000000, reward total was -18.000000. running mean: -20.198051\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.206070\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.214009\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -20.201869\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.199851\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.207852\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.215774\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.223616\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.231380\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.239066\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -20.236675\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.234308\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.241965\n",
            "resetting env. episode 1032.000000, reward total was -20.000000. running mean: -20.239546\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.247150\n",
            "resetting env. episode 1034.000000, reward total was -20.000000. running mean: -20.244679\n",
            "resetting env. episode 1035.000000, reward total was -19.000000. running mean: -20.232232\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -20.229910\n",
            "resetting env. episode 1037.000000, reward total was -20.000000. running mean: -20.227611\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.235334\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.242981\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.250551\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.258046\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -20.255465\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.262911\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.260282\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.257679\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.255102\n",
            "resetting env. episode 1047.000000, reward total was -19.000000. running mean: -20.242551\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -20.240125\n",
            "resetting env. episode 1049.000000, reward total was -21.000000. running mean: -20.247724\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.245247\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.242794\n",
            "resetting env. episode 1052.000000, reward total was -18.000000. running mean: -20.220366\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -20.218163\n",
            "resetting env. episode 1054.000000, reward total was -18.000000. running mean: -20.195981\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.194021\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.192081\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -20.180160\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.188359\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.186475\n",
            "resetting env. episode 1060.000000, reward total was -19.000000. running mean: -20.174610\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.182864\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.191036\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.189125\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.197234\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -20.195262\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.193309\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.191376\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.189462\n",
            "resetting env. episode 1069.000000, reward total was -19.000000. running mean: -20.177568\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.175792\n",
            "resetting env. episode 1071.000000, reward total was -20.000000. running mean: -20.174034\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.182294\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.180471\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.188666\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.196779\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.204812\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.212763\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.220636\n",
            "resetting env. episode 1079.000000, reward total was -18.000000. running mean: -20.198429\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -20.186445\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.194581\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.202635\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.210609\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.218503\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.216317\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.224154\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.231913\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.239594\n",
            "resetting env. episode 1089.000000, reward total was -18.000000. running mean: -20.217198\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.225026\n",
            "resetting env. episode 1091.000000, reward total was -19.000000. running mean: -20.212775\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.200648\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.208641\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.206555\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.204489\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.212444\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.220320\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.228117\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.235836\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.243477\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -20.231042\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.238732\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.246345\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.253881\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.261342\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.268729\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.276042\n",
            "resetting env. episode 1108.000000, reward total was -19.000000. running mean: -20.263281\n",
            "resetting env. episode 1109.000000, reward total was -19.000000. running mean: -20.250648\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.258142\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.255561\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.253005\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.260475\n",
            "resetting env. episode 1114.000000, reward total was -18.000000. running mean: -20.237870\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.245491\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.253037\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.260506\n",
            "resetting env. episode 1118.000000, reward total was -15.000000. running mean: -20.207901\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.215822\n",
            "resetting env. episode 1120.000000, reward total was -19.000000. running mean: -20.203664\n",
            "resetting env. episode 1121.000000, reward total was -19.000000. running mean: -20.191627\n",
            "resetting env. episode 1122.000000, reward total was -18.000000. running mean: -20.169711\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.178014\n",
            "resetting env. episode 1124.000000, reward total was -18.000000. running mean: -20.156234\n",
            "resetting env. episode 1125.000000, reward total was -19.000000. running mean: -20.144671\n",
            "resetting env. episode 1126.000000, reward total was -19.000000. running mean: -20.133225\n",
            "resetting env. episode 1127.000000, reward total was -20.000000. running mean: -20.131892\n",
            "resetting env. episode 1128.000000, reward total was -19.000000. running mean: -20.120574\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.129368\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.138074\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -20.136693\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.145326\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.153873\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.162334\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.170711\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -20.159004\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.167414\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.175740\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.183982\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -20.182143\n",
            "resetting env. episode 1141.000000, reward total was -19.000000. running mean: -20.170321\n",
            "resetting env. episode 1142.000000, reward total was -20.000000. running mean: -20.168618\n",
            "resetting env. episode 1143.000000, reward total was -20.000000. running mean: -20.166932\n",
            "resetting env. episode 1144.000000, reward total was -19.000000. running mean: -20.155262\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.153710\n",
            "resetting env. episode 1146.000000, reward total was -20.000000. running mean: -20.152173\n",
            "resetting env. episode 1147.000000, reward total was -21.000000. running mean: -20.160651\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.159044\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.167454\n",
            "resetting env. episode 1150.000000, reward total was -19.000000. running mean: -20.155779\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.164222\n",
            "resetting env. episode 1152.000000, reward total was -17.000000. running mean: -20.132579\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.141254\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.149841\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -20.148343\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.156859\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.165291\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.173638\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -20.161901\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.170282\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -20.168580\n",
            "resetting env. episode 1162.000000, reward total was -21.000000. running mean: -20.176894\n",
            "resetting env. episode 1163.000000, reward total was -19.000000. running mean: -20.165125\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -20.153474\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.151939\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.160419\n",
            "resetting env. episode 1167.000000, reward total was -19.000000. running mean: -20.148815\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.147327\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.155854\n",
            "resetting env. episode 1170.000000, reward total was -17.000000. running mean: -20.124295\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -20.123052\n",
            "resetting env. episode 1172.000000, reward total was -20.000000. running mean: -20.121822\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.130604\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.139298\n",
            "resetting env. episode 1175.000000, reward total was -19.000000. running mean: -20.127905\n",
            "resetting env. episode 1176.000000, reward total was -18.000000. running mean: -20.106626\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.115559\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -20.114404\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.113260\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.122127\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.130906\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -20.139597\n",
            "resetting env. episode 1183.000000, reward total was -19.000000. running mean: -20.128201\n",
            "resetting env. episode 1184.000000, reward total was -19.000000. running mean: -20.116919\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.115750\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.124592\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -20.123346\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.132113\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.140792\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.149384\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.147890\n",
            "resetting env. episode 1192.000000, reward total was -20.000000. running mean: -20.146411\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.154947\n",
            "resetting env. episode 1194.000000, reward total was -18.000000. running mean: -20.133397\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.142063\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.150643\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.159136\n",
            "resetting env. episode 1198.000000, reward total was -17.000000. running mean: -20.127545\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.136270\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.134907\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.143558\n",
            "resetting env. episode 1202.000000, reward total was -20.000000. running mean: -20.142122\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.150701\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.149194\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.157702\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.156125\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.144564\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.153118\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.161587\n",
            "resetting env. episode 1210.000000, reward total was -20.000000. running mean: -20.159971\n",
            "resetting env. episode 1211.000000, reward total was -20.000000. running mean: -20.158371\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.166788\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.175120\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.173369\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.181635\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.179819\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.178020\n",
            "resetting env. episode 1218.000000, reward total was -19.000000. running mean: -20.166240\n",
            "resetting env. episode 1219.000000, reward total was -20.000000. running mean: -20.164578\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.162932\n",
            "resetting env. episode 1221.000000, reward total was -19.000000. running mean: -20.151303\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -20.139790\n",
            "resetting env. episode 1223.000000, reward total was -20.000000. running mean: -20.138392\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.147008\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.145538\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.154082\n",
            "resetting env. episode 1227.000000, reward total was -19.000000. running mean: -20.142542\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.141116\n",
            "resetting env. episode 1229.000000, reward total was -17.000000. running mean: -20.109705\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.118608\n",
            "resetting env. episode 1231.000000, reward total was -20.000000. running mean: -20.117422\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -20.126248\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.124985\n",
            "resetting env. episode 1234.000000, reward total was -19.000000. running mean: -20.113735\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.122598\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.131372\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.140058\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -20.138658\n",
            "resetting env. episode 1239.000000, reward total was -19.000000. running mean: -20.127271\n",
            "resetting env. episode 1240.000000, reward total was -20.000000. running mean: -20.125998\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.134738\n",
            "resetting env. episode 1242.000000, reward total was -19.000000. running mean: -20.123391\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.122157\n",
            "resetting env. episode 1244.000000, reward total was -19.000000. running mean: -20.110935\n",
            "resetting env. episode 1245.000000, reward total was -19.000000. running mean: -20.099826\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.108828\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.117740\n",
            "resetting env. episode 1248.000000, reward total was -18.000000. running mean: -20.096562\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.105597\n",
            "resetting env. episode 1250.000000, reward total was -20.000000. running mean: -20.104541\n",
            "resetting env. episode 1251.000000, reward total was -19.000000. running mean: -20.093495\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.102560\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.111535\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.110419\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -20.109315\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.118222\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.117040\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -20.115869\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.124711\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.133464\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.142129\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -20.140708\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.149301\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.157808\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.156229\n",
            "resetting env. episode 1266.000000, reward total was -20.000000. running mean: -20.154667\n",
            "resetting env. episode 1267.000000, reward total was -18.000000. running mean: -20.133121\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.141789\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.140371\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.138968\n",
            "resetting env. episode 1271.000000, reward total was -19.000000. running mean: -20.127578\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.136302\n",
            "resetting env. episode 1273.000000, reward total was -20.000000. running mean: -20.134939\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -20.123590\n",
            "resetting env. episode 1275.000000, reward total was -19.000000. running mean: -20.112354\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.121230\n",
            "resetting env. episode 1277.000000, reward total was -18.000000. running mean: -20.100018\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -20.099018\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.108028\n",
            "resetting env. episode 1280.000000, reward total was -18.000000. running mean: -20.086947\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.096078\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.105117\n",
            "resetting env. episode 1283.000000, reward total was -19.000000. running mean: -20.094066\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.103125\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.112094\n",
            "resetting env. episode 1286.000000, reward total was -19.000000. running mean: -20.100973\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -20.089963\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.089064\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.088173\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.097291\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -20.096319\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.105355\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -20.104302\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.113259\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.122126\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -20.120905\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.129696\n",
            "resetting env. episode 1298.000000, reward total was -18.000000. running mean: -20.108399\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -20.097315\n",
            "resetting env. episode 1300.000000, reward total was -19.000000. running mean: -20.086342\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -20.075478\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.084724\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -20.073876\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.083138\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -20.082306\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.091483\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.100568\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.109563\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.108467\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.107382\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.116308\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.125145\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.123894\n",
            "resetting env. episode 1314.000000, reward total was -19.000000. running mean: -20.112655\n",
            "resetting env. episode 1315.000000, reward total was -20.000000. running mean: -20.111528\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.110413\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.109309\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.118216\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.117034\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.125863\n",
            "resetting env. episode 1321.000000, reward total was -18.000000. running mean: -20.104605\n",
            "resetting env. episode 1322.000000, reward total was -20.000000. running mean: -20.103559\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.102523\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.111498\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.110383\n",
            "resetting env. episode 1326.000000, reward total was -20.000000. running mean: -20.109279\n",
            "resetting env. episode 1327.000000, reward total was -19.000000. running mean: -20.098186\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.107204\n",
            "resetting env. episode 1329.000000, reward total was -17.000000. running mean: -20.076132\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.085371\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -20.084517\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.093672\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.102736\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.111708\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.120591\n",
            "resetting env. episode 1336.000000, reward total was -21.000000. running mean: -20.129385\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.138091\n",
            "resetting env. episode 1338.000000, reward total was -18.000000. running mean: -20.116710\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.125543\n",
            "resetting env. episode 1340.000000, reward total was -18.000000. running mean: -20.104288\n",
            "resetting env. episode 1341.000000, reward total was -18.000000. running mean: -20.083245\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.082413\n",
            "resetting env. episode 1343.000000, reward total was -19.000000. running mean: -20.071588\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.080873\n",
            "resetting env. episode 1345.000000, reward total was -18.000000. running mean: -20.060064\n",
            "resetting env. episode 1346.000000, reward total was -19.000000. running mean: -20.049463\n",
            "resetting env. episode 1347.000000, reward total was -20.000000. running mean: -20.048969\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.048479\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.047994\n",
            "resetting env. episode 1350.000000, reward total was -17.000000. running mean: -20.017514\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.027339\n",
            "resetting env. episode 1352.000000, reward total was -19.000000. running mean: -20.017066\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -20.016895\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.026726\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.026459\n",
            "resetting env. episode 1356.000000, reward total was -19.000000. running mean: -20.016194\n",
            "resetting env. episode 1357.000000, reward total was -20.000000. running mean: -20.016032\n",
            "resetting env. episode 1358.000000, reward total was -18.000000. running mean: -19.995872\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.005913\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.015854\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.015695\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.025539\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.025283\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -20.015030\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.024880\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.034631\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.044285\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.053842\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.063304\n",
            "resetting env. episode 1370.000000, reward total was -19.000000. running mean: -20.052671\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.062144\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.071522\n",
            "resetting env. episode 1373.000000, reward total was -19.000000. running mean: -20.060807\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.070199\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.079497\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.078702\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.077915\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.087136\n",
            "resetting env. episode 1379.000000, reward total was -19.000000. running mean: -20.076265\n",
            "resetting env. episode 1380.000000, reward total was -21.000000. running mean: -20.085502\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.084647\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -20.083801\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.092963\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.102033\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.111013\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.109902\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.108803\n",
            "resetting env. episode 1388.000000, reward total was -19.000000. running mean: -20.097715\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.106738\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -20.105671\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -20.104614\n",
            "resetting env. episode 1392.000000, reward total was -19.000000. running mean: -20.093568\n",
            "resetting env. episode 1393.000000, reward total was -19.000000. running mean: -20.082632\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.081806\n",
            "resetting env. episode 1395.000000, reward total was -19.000000. running mean: -20.070988\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.080278\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.089475\n",
            "resetting env. episode 1398.000000, reward total was -21.000000. running mean: -20.098581\n",
            "resetting env. episode 1399.000000, reward total was -18.000000. running mean: -20.077595\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.086819\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.095951\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.104991\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -20.103941\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.112902\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.121773\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.130555\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.139249\n",
            "resetting env. episode 1408.000000, reward total was -19.000000. running mean: -20.127857\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -20.126578\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -20.125313\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.134059\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -20.132719\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.141392\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.139978\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.148578\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.157092\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.165521\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.173866\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.182127\n",
            "resetting env. episode 1420.000000, reward total was -19.000000. running mean: -20.170306\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.168603\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -20.156917\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.165348\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.173694\n",
            "resetting env. episode 1425.000000, reward total was -18.000000. running mean: -20.151957\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.160438\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.168834\n",
            "resetting env. episode 1428.000000, reward total was -17.000000. running mean: -20.137145\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.135774\n",
            "resetting env. episode 1430.000000, reward total was -17.000000. running mean: -20.104416\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.113372\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -20.112238\n",
            "resetting env. episode 1433.000000, reward total was -20.000000. running mean: -20.111116\n",
            "resetting env. episode 1434.000000, reward total was -19.000000. running mean: -20.100005\n",
            "resetting env. episode 1435.000000, reward total was -18.000000. running mean: -20.079005\n",
            "resetting env. episode 1436.000000, reward total was -19.000000. running mean: -20.068214\n",
            "resetting env. episode 1437.000000, reward total was -21.000000. running mean: -20.077532\n",
            "resetting env. episode 1438.000000, reward total was -18.000000. running mean: -20.056757\n",
            "resetting env. episode 1439.000000, reward total was -19.000000. running mean: -20.046189\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -20.045728\n",
            "resetting env. episode 1441.000000, reward total was -19.000000. running mean: -20.035270\n",
            "resetting env. episode 1442.000000, reward total was -19.000000. running mean: -20.024918\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.034668\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -20.024322\n",
            "resetting env. episode 1445.000000, reward total was -21.000000. running mean: -20.034078\n",
            "resetting env. episode 1446.000000, reward total was -20.000000. running mean: -20.033738\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.043400\n",
            "resetting env. episode 1448.000000, reward total was -20.000000. running mean: -20.042966\n",
            "resetting env. episode 1449.000000, reward total was -17.000000. running mean: -20.012537\n",
            "resetting env. episode 1450.000000, reward total was -19.000000. running mean: -20.002411\n",
            "resetting env. episode 1451.000000, reward total was -20.000000. running mean: -20.002387\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.012363\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.022240\n",
            "resetting env. episode 1454.000000, reward total was -19.000000. running mean: -20.012017\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -20.011897\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.021778\n",
            "resetting env. episode 1457.000000, reward total was -18.000000. running mean: -20.001560\n",
            "resetting env. episode 1458.000000, reward total was -20.000000. running mean: -20.001545\n",
            "resetting env. episode 1459.000000, reward total was -21.000000. running mean: -20.011529\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.021414\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -20.021200\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -20.020988\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.030778\n",
            "resetting env. episode 1464.000000, reward total was -19.000000. running mean: -20.020470\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.030266\n",
            "resetting env. episode 1466.000000, reward total was -18.000000. running mean: -20.009963\n",
            "resetting env. episode 1467.000000, reward total was -18.000000. running mean: -19.989863\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -19.989965\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -19.990065\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -19.990164\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.000263\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.010260\n",
            "resetting env. episode 1473.000000, reward total was -19.000000. running mean: -20.000157\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.010156\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.020054\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.019854\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -20.009655\n",
            "resetting env. episode 1478.000000, reward total was -19.000000. running mean: -19.999559\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.009563\n",
            "resetting env. episode 1480.000000, reward total was -20.000000. running mean: -20.009467\n",
            "resetting env. episode 1481.000000, reward total was -20.000000. running mean: -20.009373\n",
            "resetting env. episode 1482.000000, reward total was -18.000000. running mean: -19.989279\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -19.989386\n",
            "resetting env. episode 1484.000000, reward total was -18.000000. running mean: -19.969492\n",
            "resetting env. episode 1485.000000, reward total was -20.000000. running mean: -19.969797\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -19.980099\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -19.990298\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -19.990396\n",
            "resetting env. episode 1489.000000, reward total was -16.000000. running mean: -19.950492\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -19.950987\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -19.961477\n",
            "resetting env. episode 1492.000000, reward total was -18.000000. running mean: -19.941862\n",
            "resetting env. episode 1493.000000, reward total was -20.000000. running mean: -19.942443\n",
            "resetting env. episode 1494.000000, reward total was -17.000000. running mean: -19.913019\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -19.913889\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -19.914750\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -19.905602\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -19.916546\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -19.917381\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -19.908207\n",
            "CPU times: user 2h 2min 3s, sys: 38min 44s, total: 2h 40min 47s\n",
            "Wall time: 1h 23min 6s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "4eacbb13-ef79-4089-937a-011cf8af28b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}